---
title: "Actuarial 3 Tarea 3"
subtitle: "Actuarial 3"
lang: es
author: "Marcelino Sánchez"
date: today
format:
  html:
    page-layout: full
    embed-resources: true
editor_options: 
  chunk_output_type: console
---

# Ejercicio 2: Empalmes

Para un seguro de daños, se conoce que las pérdidas menores a $c$ siguen una distribución $Exp(\frac{1}{\theta})$ con $\theta > 0$, y las pérdidas mayores o iguales a $c$ tienen una distribución $Pareto(\alpha, \beta)$ donde $\alpha > 0$ y $\beta >0$.

a) Se pide demostrar que la función de densidad de la Distribución de Empalme está dada por:

$f_X(x) = p \frac{\theta^{-1}e^{-\frac{x}{\theta}}}{1-e^{-\frac{c}{\theta}}} I(0 < x < c) + (1-p) \frac{\alpha (c+\beta)^{\alpha}}{(x+\beta)^{\alpha + 1}} I(x \geq c)$

Dado:

- $\theta = 100$
- $\alpha = 4$
- $\beta = 200$
- $c = 100$

Se pide:

i) Graficar $f_X(x)$ con $p = 0.6$. Determinar si $f_X(x)$ es continua en $x = 100$.

```{r, echo=FALSE, message=FALSE, warning=FALSE}
# Instala e importa las librerías necesarias
library(ggplot2)

# Parámetros dados
p <- 0.6
theta <- 100  
alpha <- 4  
beta <- 200   
c <- 100      

# Definición de la función f_X(x)
f_X <- function(x) {
  if(x < c & x > 0){
    resultado = p * (theta^(-1) * exp(-x/theta))/(1 - exp(-c/theta))}
  else if (x >= c ){
    resultado = (1-p) * (alpha * (c+beta)^alpha)/((x + beta)^(alpha+1))
  }else{
    resultado = 0
  }
  return(resultado)
}

# Genera valores para x y f_X(x)
x_vals <- seq(.001, c-.0001, length.out = 1000)  # Modifica el rango de x si es necesario
y_vals <- sapply(x_vals, f_X)

# Grafica la función
df1 <- data.frame(x = x_vals, y = y_vals)

x_vals <- seq(c+.001, 300, length.out = 1000)  # Modifica el rango de x si es necesario
y_vals <- sapply(x_vals, f_X)

df2 <- data.frame(x = x_vals, y = y_vals)

x_vals <- seq(-50, -.001, length.out = 1000)  # Modifica el rango de x si es necesario
y_vals <- sapply(x_vals, f_X)

df3 <- data.frame(x = x_vals, y = y_vals)
df1$y[df1$x == c] <- NA
df2$y[df2$x == c] <- NA
df3$y[df3$x == c] <- NA

ggplot() + 
  geom_line(data=df1, aes(x, y)) +
  geom_line(data=df2, aes(x, y)) +
  geom_line(data=df3, aes(x, y)) +
  geom_point(aes(x = 0, y = f_X(0)), color = "red", size = 3 ) +
  geom_point(aes(x = c, y = f_X(c)), color = "red", size = 3) +
  ggtitle(expression(f[X](x))) +
  geom_point(aes(x = 0, y = f_X(0.001)), color = "red", size = 3 , shape=1) +
  geom_point(aes(x = c, y = f_X(c-.001)), color = "red", size = 3, shape=1) +
  ggtitle(expression(f[X](x))) +
  theme_minimal()

```

ii) Encontrar el valor de $ p $ para que $ f_X(x) $ sea continua en $ x = 100 $ y luego graficar $ f_X(x) $ con este valor.

```{r}
theta <- 100  
alpha <- 4  
beta <- 200   
c <- 100 
aux1 <- ((c+beta)*(theta^(-1))*((exp(-c/theta))/(1-exp(-c/theta)))
 + alpha)
pvalor <- alpha/aux1
```

El valor de p en el cual se satisface la continuidad en c es `r pvalor`

```{r, echo=FALSE, message=FALSE, warning=FALSE}
p <- pvalor
# Genera valores para x y f_X(x)
x_vals <- seq(.001, c-.0001, length.out = 1000)  # Modifica el rango de x si es necesario
y_vals <- sapply(x_vals, f_X)

# Grafica la función
df1 <- data.frame(x = x_vals, y = y_vals)

x_vals <- seq(c+.001, 300, length.out = 1000)  # Modifica el rango de x si es necesario
y_vals <- sapply(x_vals, f_X)

df2 <- data.frame(x = x_vals, y = y_vals)

x_vals <- seq(-50, -.001, length.out = 1000)  # Modifica el rango de x si es necesario
y_vals <- sapply(x_vals, f_X)

df3 <- data.frame(x = x_vals, y = y_vals)
df1$y[df1$x == c] <- NA
df2$y[df2$x == c] <- NA
df3$y[df3$x == c] <- NA

ggplot() + 
  geom_line(data=df1, aes(x, y)) +
  geom_line(data=df2, aes(x, y)) +
  geom_line(data=df3, aes(x, y)) +
  geom_point(aes(x = 0, y = f_X(0)), color = "red", size = 3 ) +
  geom_point(aes(x = c, y = f_X(c)), color = "red", size = 3) +
  ggtitle(expression(f[X](x))) +
  geom_point(aes(x = 0, y = f_X(0.001)), color = "red", size = 3 , shape=1) +
  geom_point(aes(x = c, y = f_X(c-.001)), color = "red", size = 3, shape=1) +
  ggtitle(expression(f[X](x))) +
  theme_minimal()

```



# Ejercicio 4: Colas de las distribuciones

Suponga que $X \sim \text{Pareto}(\alpha, \theta)$ y  $Y \sim \text{Gamma}(\tau, \beta)$.


## Comparación de distribuciones

b) Para comparar de manera más justa ambas distribuciones, queremos que ambas tengan media 5 y varianza 75.

i) Encuentre los valores de $\alpha$, $\theta$, $\tau$, y $\beta$.

```{r}

#Calculamos los parámetros de la pareto
varianza <- 75

esperanza <- 5

aux1 <- varianza/(esperanza^2) +1

aux2 <- varianza/(esperanza^3) -1/esperanza

theta <- aux1/aux2

alpha <- theta/esperanza + 1

#Calculamos los parámetros de la gamma

tau <- (esperanza^2)/varianza

beta <- varianza/esperanza

```

Los valores de los parámetros son:

- $\alpha = `r alpha`$
- $\theta = `r theta`$
- $\tau = `r tau`$
- $\beta = `r beta`$


ii) Compare en la misma gráfica ambas funciones de densidad. ¿Qué conclusiones se pueden derivar?

```{r, echo=FALSE, message=FALSE, warning=FALSE}
library(actuar)

library(ggplot2)

# Define parameters for the Pareto distribution
b <- alpha
scale_pareto <- theta

# Define parameters for the Gamma distribution
a <- tau
scale_gamma <- beta

# Create x values
x <- seq(0, 5, length.out = 1000)

# Compute PDFs
pdf_pareto <- dpareto(x, shape = b, scale = scale_pareto)
pdf_gamma <- dgamma(x, shape = a, scale = scale_gamma)

# Create data frame for ggplot
df <- data.frame(x, pdf_pareto, pdf_gamma)

# Plot both distributions on the same graph using ggplot
ggplot(df, aes(x)) + 
  geom_line(aes(y = pdf_pareto, color = paste("Pareto b=", b))) +
  geom_line(aes(y = pdf_gamma, color = paste("Gamma a=", a, ", scale=", scale_gamma))) +
  labs(title = "Pareto vs Gamma Distributions", y = "Density", color = "Distribution") +
  theme_minimal() +
  coord_cartesian(ylim = c(0, 0.5))  # set y-axis limit here

```


Se puede derivar que la gamma acumula más que la pareto para valores cercanos a cero (aún teniendo la misma varianza y media), por lo que la pareto tiene cola más pesada que la gamma.

# Ejercicio 5

Verosimilitud para datos agrupados. Suponga que las pérdidas de cierto riesgo siguen una distribución con función de distribución acumulada $F(x) = (1 - \frac{\theta}{x})I(\theta, \infty)(x)$, donde $\theta > 0$. Una muestra aleatoria de 20 siniestros contiene 9 pérdidas por debajo de 10 mil dólares, 6 pérdidas entre 10 y 25 mil dólares y 5 pérdidas que exceden los 25 mil dólares. Encuentre el estimador de máxima verosimilitud de $\theta$. [10]

Por principio de máxima verosimilitud tenemos que el logaritmo de la función de verosimilitud para datos agrupados está dada por:

$$\ln(L(\theta)) = 9 \ln\left( F(10000, \theta) - F(0, \theta) \right) + 6 \ln\left( F(25000, \theta) - F(10000, \theta) \right)$$

$$+5\ln\left( 1 - F(25000, \theta)\right)$$


```{r}

acumulada <- function(x, theta){
  if(x > theta){
    resultado = 1 - (theta/x)
  }else{
    resultado = 0
  }
  return(resultado)
}


verosimilitud <- function(theta){
  resultado <- 9*log(acumulada(10000, theta)-acumulada(0,theta)) + 6*log(acumulada(25000, theta)-acumulada(10000,theta)) + 5*log(1-acumulada(25000,theta))
  return(resultado)
}

neg_verosimilitud <- function(theta) {
  return(-verosimilitud(theta))
}

# Usamos la función optimize para encontrar el valor de theta que maximiza la verosimilitud
# Establece un rango razonable para theta basado en tu conocimiento del problema
resultado <- optimize(neg_verosimilitud, interval=c(0, 9000))

```

El valor de $\theta$ que maximiza la verosimilitud numéricamente es `r resultado$minimum` .

# Ejercicio 6

Ajuste para datos agrupados. A continuación se presenta la distribución de frecuencias de una muestra de 250 pérdidas de un seguro de hogar.

| Pérdida (miles de pesos) | Frecuencia |
|--------------------------|------------|
| 0 - 50                   | 3          |
| 50 - 150                 | 54         |
| 150 - 300                | 62         |
| 300 - 500                | 43         |
| 500 - 1,000              | 39         |
| 1,000 - 2,000            | 24         |
| 2,000 - 4,000            | 11         |
| 4,000 - 8,000            | 9          |
| 8,000 - 16,000           | 4          |
| Más de 16,000            | 1          |
| **Total**                | **250**    |

a) Construya el histograma de frecuencias relativas respectivo. Note que las clases son de distinta longitud. [10]

```{r}
# Crea un vector con los límites de las clases

clases <- c(0, 50, 150, 300, 500, 1000, 2000, 4000, 8000, 16000, 24000)

# Crea un vector con las frecuencias

frecuencias <- c(3, 54, 62, 43, 39, 24, 11, 9, 4, 1)

# Crea un vector con las frecuencias relativas

frecuencias_relativas <- frecuencias/sum(frecuencias)

# Crea un vector con las longitudes de las clases

longitudes_clases <- diff(clases)


# Crea un vector con los factores de ajuste

factores_ajuste <- 1/longitudes_clases

# Crea un vector con las frecuencias relativas ajustadas

frecuencias_relativas_ajustadas <- frecuencias_relativas * factores_ajuste

# Crea un vector con las frecuencias relativas corregidas

frecuencias_relativas_corregidas <- frecuencias_relativas_ajustadas/sum(frecuencias_relativas_ajustadas)

# Crea un vector con los puntos medios de las clases

puntos_medios_clases <- (clases[-1] + clases[-length(clases)])/2

# Crea un data frame con los datos

df <- data.frame( frecuencias, frecuencias_relativas, frecuencias_relativas_ajustadas, frecuencias_relativas_corregidas, puntos_medios_clases)


# Grafica el histograma
gg1 <-ggplot(df, aes(x = puntos_medios_clases, y = frecuencias_relativas_corregidas)) +
  geom_bar(width = longitudes_clases, stat = "identity", fill = "steelblue") +
  geom_text(aes(label = frecuencias), vjust = -0.5) +
  ggtitle("Histograma de frecuencias relativas") +
  xlab("Pérdida (miles de pesos)") +
  ylab("Frecuencia relativa") +
  theme_minimal()

gg1

ggplot(df[1:6, ], aes(x = puntos_medios_clases, y = frecuencias_relativas_corregidas)) +
  geom_bar(width = longitudes_clases[1:6], stat = "identity", fill = "steelblue") +
  geom_text(aes(label = round(frecuencias_relativas_corregidas,digits = 5)), vjust = -0.5) +
  ggtitle("Histograma de frecuencias relativas (primeras 8 clases)") +
  xlab("Pérdida (miles de pesos)") +
  ylab("Frecuencia relativa") +
  xlim(c(0, 2000)) +  # Limitar el eje x hasta un poco después de 8,000
  theme_minimal()





```



b) A partir de los datos agrupados, ajuste por máxima verosimilitud la Distribución Exponencial Inversa con función de distribución acumulada 
$F(x) = e^{-\frac{\theta}{x}} I(0,\infty)(x), \quad \theta > 0$.
Especifique $\hat{\theta}$ y grafique la densidad ajustada junto con el histograma del inciso anterior. [10]


Por principio de máxima verosiimilitud tenemos que el logaritmo de la función de verosimilitud para datos agrupados está dada por:

$$\ln(L(\theta)) = \sum_{i=1}^{10} n_i \ln\left( F(b_i, \theta) - F(a_i, \theta) \right)$$

```{r}
acumulada <- function(x, theta){
  if(x > 0){
    resultado = exp(-theta/x)
  }else{
    resultado = 0
  }
  return(resultado)
}


verosimilitud <- function(theta){
  for(i in 1:10){
    if(i == 1){
      resultado <- frecuencias[i]*log(acumulada(clases[i+1], theta)-acumulada(clases[i],theta))
    }else{
      resultado <- resultado + frecuencias[i]*log(acumulada(clases[i+1], theta)-acumulada(clases[i],theta))
    }
  }
  return(resultado)
}

neg_verosimilitud <- function(theta) {
  return(-verosimilitud(theta))
}

# Usamos la función optimize para encontrar el valor de theta que maximiza la verosimilitud
# Establece un rango razonable para theta basado en tu conocimiento del problema
resultado <- optimize(neg_verosimilitud, interval=c(0, 9000))



# Compute likelihoods

d_gamma_inv <- function(x, theta) {
  ifelse(x > 0, (theta / (x^(2))) * exp(-theta/x),0)
}

# Valores de x
x_vals <- seq(0, 24000, by = 100)

# Calcular la función de densidad para estos valores
y_vals <- sapply(x_vals, d_gamma_inv, theta =resultado$minimum )

# Graficar

data_to_plot2 <- data.frame(x = x_vals, y = y_vals)

ggplot(data_to_plot2, aes(x = x, y = y)) +
  geom_line(color = "blue") +
  ggtitle(paste("Distribución Gama Inversa: alpha =", 1, ", beta =", resultado$minimum)) +
  xlab("x") +
  ylab("f(x)") +
  theme_minimal() 

# Primero, configura el ggplot base
plot_base <- ggplot() + 
  theme_minimal() +
  xlab("x") + ylab("f(x)")

# Supongo que plot_base y los otros datasets ya están definidos

# Calcula el factor de transformación. Este factor se basará en la relación entre los rangos máximos de tus datos.
max_y_data_to_plot2 <- max(data_to_plot2$y)
max_df <- max(df$frecuencias_relativas_corregidas)

# Factor de transformación
trans_factor <- max_y_data_to_plot2 / max_df

final_plot <- plot_base +
  # Primero traza el histograma
  geom_bar(data = df, aes(x = puntos_medios_clases, y = frecuencias_relativas_corregidas, width = longitudes_clases), stat = "identity", fill = "steelblue") +
  # Luego traza la línea, pero multiplica los valores por el factor de transformación
  geom_line(data = data_to_plot2, aes(x = x, y = y / trans_factor), color = "red") +
  ggtitle(paste("Distribución Gama Inversa: alpha =", 1, ", beta =", resultado$minimum)) +
  # Añade el eje secundario, dividiendo por el factor de transformación para mostrar los valores reales
  scale_y_continuous(sec.axis = sec_axis(~./trans_factor, name = "Eje secundario"))

# Finalmente, muestra el gráfico
print(final_plot)

```

El valor de $\theta$ que maximiza la verosimilitud numéricamente es `r resultado$minimum` .

# Ejercicio 7

Ajuste de una mezcla. Con base en 180 pérdidas de un seguro de vehículos que cubre
automóviles y motocicletas se construyó el siguiente histograma:

```{r}
library(readxl)
library(kableExtra)
ruta <- paste0(getwd(), "/bases/Tarea 3.xlsx")
datosPerdida <- read_excel(ruta, sheet = "7", range = "A8:B188")

hist(datosPerdida$Pérdida)
```

Se quiere ajustar una mezcla de distribuciones Ji-Cuadradas con medias $\nu$ y $2\nu$ con ponderadores $\alpha$ y $1-\alpha$, respectivamente, donde $\nu \in \mathbb{Z^+}$ y $0 < \alpha < 1$.

**a)** A partir de los datos individuales, escriba la función de densidad de la mezcla y verifique que los EMV son $\hat{\alpha} = 0.2468$ y $\hat{\nu} = 30$. (Sugerencia: Optimice numéricamente respecto a ambos parámetros, luego fije $\hat{\nu}$ al entero más cercano y maximice nuevamente para obtener $\hat{\alpha}$). [15]

Primero notamos que la función de verosimilitud está dada por:

$$
L(\nu, \alpha)= \prod_{i=1}^{180} \left[ \alpha \frac{x^{\frac{\nu}{2}-1} e^{-\frac{x}{2}}}{2^{\frac{\nu}{2}} \Gamma\left(\frac{\nu}{2}\right)} + (1-\alpha)\frac{x^{\nu-1} e^{-\frac{x}{2}}}{2^{\nu} \Gamma(\nu)} \right]
$$

Por lo que el logaritmo de la función de verosimilitud está dado por:

$$
\ln(L(\nu, \alpha)) = \sum_{i=1}^{180} \left[ \ln\left( \alpha \frac{x^{\frac{\nu}{2}-1} e^{-\frac{x}{2}}}{2^{\frac{\nu}{2}} \Gamma\left(\frac{\nu}{2}\right)} + (1-\alpha)\frac{x^{\nu-1} e^{-\frac{x}{2}}}{2^{\nu} \Gamma(\nu)} \right) \right]
$$



```{r, warning=FALSE}
# Optimizamos numéricamente respecto a ambos parámetros

mezclaFuncion <- function(x, alpha, nu) {
  alpha * dgamma(x, shape = nu/2, scale = 2) + (1 - alpha) * dgamma(x, shape = nu, scale = 2)+.00000001
}

verosimilitud <- function(alpha, nu, datos) {
  log_mezcla <- log(mezclaFuncion(datos, alpha, nu))
  resultado <- sum(log_mezcla)
  return(resultado)
}

neg_verosimilitud <- function(params, datos) {
  alpha <- params[1]
  nu <- params[2]
  -verosimilitud(alpha, nu, datos)
}

# Suponiendo que tienes un dataframe llamado datosPerdida con una columna 'Pérdida'
datos <- datosPerdida$Pérdida

# Valores iniciales para alpha y nu
valores_iniciales <- c(alpha = 0.5, nu = 2)

# Límites para alpha y nu
# alpha está entre 0 y 1, y nu es mayor que 0
limites <- list(c(0.000001, 10), c(1e-6, 50))

# Utilizar optim con el método "L-BFGS-B" para encontrar los parámetros óptimos
resultado_optim <- optim(
  par = valores_iniciales,
  fn = neg_verosimilitud,
  datos = datos,
  method = "L-BFGS-B",
  lower = sapply(limites, `[`, 1), # Límites inferiores
  upper = sapply(limites, `[`, 2)  # Límites superiores
)

# Los parámetros optimizados
(alpha_optim <- resultado_optim$par[1])
(nu_optim <- resultado_optim$par[2])

# Optimizamos numéricamente respecto a alpha

# Ahora neg_verosimilitud aceptará solo alpha ya que nu es fijo.
neg_verosimilitud <- function(alpha, datos) {
  nu <- 30  # Fijamos nu a 30
  -verosimilitud(alpha, nu, datos)
}

# Suponiendo que tienes un dataframe llamado datosPerdida con una columna 'Pérdida'
datos <- datosPerdida$Pérdida

# Valor inicial solo para alpha
valor_inicial_alpha <- 0.5

# Límite solo para alpha
limites_alpha <- c(0.000001, 1)

# Utilizar optim con el método "L-BFGS-B" para encontrar el parámetro óptimo de alpha
resultado_optim <- optim(
  par = valor_inicial_alpha,
  fn = neg_verosimilitud,
  datos = datos,
  method = "L-BFGS-B",
  lower = limites_alpha[1], # Límite inferior para alpha
  upper = limites_alpha[2]  # Límite superior para alpha
)

# El parámetro optimizado para alpha
alpha_optim <- resultado_optim$par

```

Con lo cual obtenemos que $\hat{\alpha} = `r alpha_optim`$ y $\hat{\nu} = `r 30`$.


**b)** Grafique la función de densidad ajustada junto con el histograma de frecuencias relativas y obtenga los EMV de las modas. [15]

```{r}
# Asumiendo que alpha_optim contiene el valor de alpha optimizado
alpha_emv <- alpha_optim
nu_emv <- 30  # Valor fijo de nu

# 1. Crear una secuencia de valores sobre los que queremos calcular la densidad ajustada
x_values <- seq(min(datosPerdida$Pérdida), max(datosPerdida$Pérdida), length.out = 100)

# 2. Calcular la función de densidad para cada uno de esos valores
densidad_ajustada <- mezclaFuncion(x_values, alpha_emv, nu_emv)

# 3. Crear el histograma con frecuencias relativas
hist(datosPerdida$Pérdida, breaks = 20, probability = TRUE, col = 'gray', border = 'white',
     main = 'Histograma y Densidad Ajustada',
     xlab = 'Pérdida', ylab = 'Densidad')

# 4. Sobreponer la función de densidad ajustada
lines(x_values, densidad_ajustada, col = 'blue', lwd = 2)

# 5. Obtener las modas

# Punto de inicio para la búsqueda de la moda, podría ser el valor medio de los datos, por ejemplo
valor_inicial1 <- 60
valor_inicial2 <- 25
# Necesitamos una función que sea negativa para la función de densidad para poder minimizarla
neg_densidad_mezcla <- function(x, alpha, nu) {
  -mezclaFuncion(x, alpha, nu)
}
# Usar optim para encontrar la moda de la función de densidad
resultado_moda <- optim(
  par = valor_inicial1,
  fn = neg_densidad_mezcla,
  alpha = alpha_emv,
  nu = nu_emv,
  method = "L-BFGS-B",
  lower = min(datosPerdida$Pérdida), # Límite inferior para la búsqueda
  upper = max(datosPerdida$Pérdida)  # Límite superior para la búsqueda
)

# Usar optim para encontrar la moda de la función de densidad
resultado_moda2 <- optim(
  par = valor_inicial2,
  fn = neg_densidad_mezcla,
  alpha = alpha_emv,
  nu = nu_emv,
  method = "L-BFGS-B",
  lower = min(datosPerdida$Pérdida), # Límite inferior para la búsqueda
  upper = max(datosPerdida$Pérdida)  # Límite superior para la búsqueda
)

# La moda es el punto donde la función de densidad es máxima
moda1 <- resultado_moda$par
moda2 <- resultado_moda2$par

```

Con lo cual los valores de las modas son `r moda1` y `r moda2`.


# Ejercicio 8

Pruebas de bondad de ajuste. A continuación se muestran los montos (en miles de
dólares) de 60 siniestros correspondientes a una póliza de incendio:

**a)** Construya un histograma de frecuencias relativas con 6 clases de igual longitud entre $30,000$ y $90,000$ dólares. ¿Qué distribuciones paramétricas podrían ajustar a este perfil de siniestros? [10]

```{r}
library(readxl)
library(kableExtra)
ruta <- paste0(getwd(), "/bases/Tarea 3.xlsx")
datosPerdida <- read_excel(ruta, sheet = "8", range = "L6:M66")

hist(datosPerdida$Monto, 
     breaks = seq(30, 90, length.out = 7), # Define los límites de las clases
     freq = FALSE, # Usa frecuencias relativas en lugar de frecuencias absolutas
     main = "Histograma de Frecuencias Relativas", 
     xlab = "Monto de Pérdida ($)", 
     ylab = "Densidad", 
     col = "lightblue", 
     border = "black")
```

**b)** Si se decide ajustar una Distribución Lognormal con parámetros $\mu \in \mathbb{R}$ y $\sigma^2 > 0$, calcule sus respectivos EMV. Grafique la función de distribución acumulada ajustada junto con la Ojiva. ¿Qué puede concluir? (Sugerencia: Recuerde que estos estimadores fueron deducidos en el Ejercicio E19). [10]

Primero notemos que la función de verosimilitud es de la forma:

$$
L(\mu, \sigma \mid x_1, \ldots, x_n) = \prod_{i=1}^{n} \frac{1}{x_i \sigma \sqrt{2\pi}} \exp\left(-\frac{(\ln x_i - \mu)^2}{2\sigma^2}\right)
$$

y la log-verosimilitud es de la forma:

$$
\ln L(\mu, \sigma \mid \underline{X}_{n}) = -n\ln(\sigma) - \frac{n}{2}\ln(2\pi) - \sum_{i=1}^{n}\ln(x_i) - \frac{1}{2\sigma^2}\sum_{i=1}^{n}(\ln x_i - \mu)^2
$$

```{r}
# Optimizamos numéricamente respecto a ambos parámetros

lognormal <- function(x, mu, sigma) {
  dlnorm(x, meanlog = mu, sdlog = sigma)
}

verosimilitud <- function(mu, sigma, datos) {
  log_ver_ind <- log(lognormal(datos, mu, sigma)+.00000001)
  resultado <- sum(log_ver_ind)
  return(resultado)
}

neg_verosimilitud <- function(params, datos) {
  mu <- params[1]
  sigma <- params[2]
  -verosimilitud(mu, sigma, datos)
}

# Suponiendo que tienes un dataframe llamado datosPerdida con una columna 'Pérdida'
datos <- datosPerdida$Monto

# Valores iniciales para alpha y nu
valores_iniciales <- c(mu = log(40), sigma = 10)

# Límites para alpha y nu
# alpha está entre 0 y 1, y nu es mayor que 0
limites <- list(c(.00001, 90), c(.00001, 90))

# Utilizar optim con el método "L-BFGS-B" para encontrar los parámetros óptimos
resultado_optim <- optim(
  par = valores_iniciales,
  fn = neg_verosimilitud,
  datos = datos,
  method = "L-BFGS-B",
  lower = sapply(limites, `[`, 1), # Límites inferiores
  upper = sapply(limites, `[`, 2)  # Límites superiores
)

# Los parámetros optimizados
mu_optim <- resultado_optim$par[1]
sigma_optim <- resultado_optim$par[2]

```

Con lo cual obtenemos que $\hat{\mu} = `r mu_optim`$ y $\hat{\sigma^{2}} = `r sigma_optim^2`$.


Ahora procederemos a graficar la función de distribución acumulada ajustada junto con la Ojiva.

```{r}
mu_est <- mu_optim
sigma_est <- sigma_optim

# Define una secuencia de valores para los cuales se calculará la ojiva
x_values <- seq(0,100, length.out = 1000)
x_values2 <- seq(30,89, length.out = 1000)
ecdf_datos <- ecdf(datosPerdida$Monto)
# Función de la ojiva

ojiva_lognormal <- function(x){
  particion <- seq(30, 90, length.out = 7)
  for (i in particion){
    if(x <= i & x >= i-10){
      cparticionBefore <- i-10
      cparticionNow <- i
      resultado <-  (cparticionNow - x)*ecdf_datos(cparticionBefore)/(cparticionNow- cparticionBefore) + (x - cparticionBefore)*ecdf_datos(cparticionNow)/(cparticionNow- cparticionBefore)
      break
    }
  } 
  return(resultado)
}
ojiva_empirica_values <- sapply(x_values2, ojiva_lognormal)

lognormal <- function(x) {
  plnorm(x, meanlog = mu_est, sdlog = sigma_est)
}

library(ggplot2)
# Crear el gráfico con ggplot2
ggplot() +
  geom_line(aes(x= x_values, y = lognormal(x_values)), color = "blue") +
  geom_line(aes(x=x_values2, y = ojiva_empirica_values), color = "red") +
  labs(title = 'Log-normal CDF and Empirical CDF', x = 'Monto', y = 'CDF') +
  theme_minimal() +
  theme(legend.position = "bottomright")


```

Tenemos que tomar en cuenta que la ojiva es un estimador de la función de distribución acumulada similar al histograma. Y al paracer nuestra distribución ajustada sí está pareciéndose a la distribución de los datos. Solo cabe resaltar que subestima abtes de los 50,000 y sobreestima después de los 50,000.

**c)** Construya la Gráfica de Probabilidad para el ajuste del inciso b). ¿Qué puede concluir? [10]

```{r}
# Carga la librería necesaria para funciones adicionales si es necesario
# Por ejemplo, para la distribución t, necesitas la función qt de la librería stats
library(stats)

# Genera tus datos
datos <- datosPerdida$Monto

# Distribución empírica
dist_empirica <- ecdf(datos)
datos_empirica <- dist_empirica(datos)
# Distribución teórica
datos_teoricos <- lognormal(datos)

data <- data.frame(datos_empirica, datos_teoricos)

# Crea la gráfica de probabilidad
ggplot(data,aes(x=datos_empirica , y= datos_teoricos)) +
  geom_point() +
  geom_abline(slope = 1, intercept = 0, color = "red") +
  labs(title = "Gráfica de Probabilidad", x = "Distribución Teórica", y = "Distribución Empírica") +
  theme_minimal()

```

Con esta gráfica es mucho más claro y preciso poder decir que la nuestra distribución ajustada se parece a la de los datos muestreados y por tanto no parecen haber grandes desviaciones. Fue un buen ajuste.

**d)** Aplique la Prueba Kolmogorov-Smirnov para validar el ajuste Lognormal del inciso b). Considere un tamaño de error tipo I del 5%. [10]

```{r}
# Realizar la prueba de Kolmogorov-Smirnov
ks_result <- ks.test(datosPerdida$Monto, "plnorm", meanlog = mu_est, sdlog = sigma_est)

# Imprimir el resultado
print(ks_result)
```

Con lo cual no se rechaza la hipótesis nula de que la distribución de los datos es lognormal, es decir, el ajuste fue bueno estadísticamente.

**e)** Aplique la Prueba Ji-Cuadrada para validar el ajuste Lognormal del inciso b). Considere la partición del inciso a) y agregue 2 clases (inicial y final) para abarcar todo el soporte de la Lognormal. Calcule el valor-p para concluir. [10]

```{r}
mis_datos <- datosPerdida$Monto
# Crear una secuencia de intervalos para la tabla de frecuencia
breaks1 <- seq(30, 90, length.out = 7)  # Ajusta el número de intervalos según sea necesario
breaks <- c(-Inf, breaks1, Inf)

# Frecuencias observadas: conteo de datos en cada intervalo
observed <- hist(mis_datos, breaks = breaks, plot = FALSE)$counts

# probabilidades
probabilities <- plnorm(breaks, meanlog = mu_est, sdlog = sigma_est)
diff_prob <- diff(probabilities)

# Realizar la prueba chi-cuadrada
pruebachi <- chisq.test(x = observed, p = diff_prob, rescale.p = TRUE)
kparam <- length(diff_prob) # Número de cajitas
qparam <- 2 # parámetros de la distribución ajustada
df_chi <- kparam - qparam - 1 # Grados de libertad
pvalue <- pchisq(pruebachi$statistic, df = df_chi, lower.tail = FALSE)  # Valor-p
```

Por lo que el estadístico de prueba es `r pruebachi$statistic` y el valor-p es `r pvalue`. Es decir, no hay suficiente evidencia para rechazar la hipótesis nula de que la distribución de los datos es lognormal. Otra vez fue un buen ajuste.

# Ejercicio 9

Selección de modelos. A continuación se muestra el diagrama de tallo y hojas del monto
de las reclamaciones de un seguro que cubre daños ocasionados por fenómenos
hidrometeorológicos:

```{r}
datos <- read_excel(ruta, sheet = "9", range = "M7:N23")
```

a) Suponga que se quiere ajustar una Distribución Gamma con media $\mu = \alpha\theta$.

i) Calcule los EMV de $\alpha$ y $\theta$. (Sugerencia: Recuerde que en el Ejercicio E14 se obtuvieron estos estimadores o utilice EViews para obtenerlos). [5]

Recordemos que la log de la función de máxima verosimilitud está dada por:

$$\ln L(\alpha, \theta) = (\alpha -1 )*\sum_{i=1}^{n}\ln (x_i)- \sum_{i=1}^{n} \frac{x_{i}}{\theta} -n \ln(\Gamma(\alpha))-n\alpha \ln(\theta)$$

Y derivando parcialmente con respecto a cada parámetro obtenemos las siguientes ecuaciones:

$$\sum_{i=1}^{n}\ln (x_i)-n\frac{\Gamma'(\alpha)}{\Gamma(\alpha)}-n\ln(\theta)=0$$

$$\frac{\sum_{i=1}^{n}x_i}{\theta^{2}}-n\frac{\alpha}{\theta}=0$$

Con lo cual simplificando obtenemos que 

$$\theta=\frac{\bar{x}}{\alpha}$$

$$\sum_{i=1}^{n}\ln (x_i)-n\frac{\Gamma'(\alpha)}{\Gamma(\alpha)}-n\ln(\theta)=0$$

Es decir, solo hay que utilizar un método numérico para encontrar el valor de $\alpha$ que cumple la segunda ecuación y con ese valor encontrar el valor de $\theta$.

```{r}
# Definir la función que representa la segunda ecuación
funcion_objetivo <- function(alpha, x) {
  n <- length(x)
  sum(log(x)) - n * digamma(alpha) - n * log(mean(x) / alpha)
}

# Usar uniroot para encontrar el valor de alpha
# Es importante dar un intervalo donde se espera encontrar la raíz (alpha)
resultado <- uniroot(funcion_objetivo, interval = c(0.001, 100), x = datos$Monto)

# El valor de alpha
alpha_estimado <- resultado$root

# Ahora calcular theta usando la relación dada
theta_estimado <- mean(datos$Monto) / alpha_estimado

```

Con lo cual los valores de $\hat{\alpha}$ y $\hat{\theta}$ son `r alpha_estimado` y `r theta_estimado` respectivamente.

ii) Obtenga el Gráfico Cuantil-Cuantil para el ajuste de la distribución Gamma. ¿Qué puede concluir? (Sugerencia: Utilice EViews). [5]

```{r}
library(ggplot2)

# Ordena tus datos y calcula los cuantiles empíricos
datos$empirical <- sort(datos$Monto)
datos$theoretical <- qgamma(ppoints(length(datos$Monto)), shape = alpha_estimado, rate = theta_estimado)

# Crea el gráfico Q-Q
ggplot(datos, aes(x = theoretical, y = empirical)) +
  geom_point() +
  geom_abline(slope = 1, intercept = 0, color = "red", linetype = "dashed") +
  labs(title = "Q-Q Plot for Gamma Distribution",
       x = "Theoretical Quantiles",
       y = "Sample Quantiles") +
  theme_minimal()



```

AJUSTAR QQPLOT(CHECAR)

iii) Aplique la Prueba de Razón de Verosimilitudes con un nivel de significancia del 5% para determinar si el monto de las reclamaciones corresponde a una Gamma con media $\mu_0 = 2$. (Sugerencia: Note que a pesar de que la distribución Gamma tiene 2 parámetros, la prueba se debe hacer sólo con un parámetro pues $T(\mu) = (\alpha, \theta) = \alpha\theta$, con $T: \mathbb{R}^2 \rightarrow \mathbb{R}$. No olvide obtener el EMV restringido considerando el valor de $H_0$). [15]

```{r}
logverosimilitud <- function(alpha, theta, x) {
  n <- length(x)
  resultado <- (alpha - 1) * sum(log(x)) - sum(x / theta) - n * log(gamma(alpha)) - n * alpha * log(theta)
  return(resultado)
}

logverosimilitud_restringida <- function(alpha, x) {
  n <- length(x)
  theta <- 2 / alpha
  resultado <- (alpha - 1) * sum(log(x)) - sum(x / theta) - n * log(gamma(alpha)) - n * alpha * log(theta)
  return(resultado)
}

# Obtener el EMV restringido

neg_logverosimilitud_restringida <- function(alpha, x) {
  -logverosimilitud_restringida(alpha, x)
}

resultado_optim <- optim(
  par = 1,
  fn = neg_logverosimilitud_restringida,
  x = datos$Monto,
  method = "L-BFGS-B",
  lower = 0.00001, # Límite inferior para alpha
  upper = 100  # Límite superior para alpha
)

alpha_restringido <- resultado_optim$par
theta_restringido <- 2 / alpha_restringido

logverosimilitud_restringida <- logverosimilitud(alpha_restringido, theta_restringido, datos$Monto)

logverosimilitud_original <- logverosimilitud(alpha_estimado, theta_estimado, datos$Monto)

LR_test <- 2 * (logverosimilitud_original -  logverosimilitud_restringida)

valorptest <- 1 - pchisq(LR_test, df = 1)

```

Con lo cual obtuvimos que el valor de la razón de verosimilitudes es `r LR_test` y el valor-p es `r valorptest`. Por lo que no se rechaza la hipótesis nula de que el monto de las reclamaciones corresponde a una Gamma con media $\mu_0 = 2$.

b) Suponga que se quiere ajustar una distribución Weibull $(\gamma, \theta)$, $\gamma > 0$ y $\theta > 0$.

Primero notemos que la función de verosimilitud está dada por:

$$L(\theta, \gamma)= \frac{\gamma^{n}}{\theta^{\gamma n}}\left(\Pi_{i=1}^{n}x_{i}\right)^{\gamma-1}e^{-\sum_{i=1}^{n}(\frac{x_{i}}{\theta})^{\gamma}}$$

Con lo cual la log-verosimilitud está dada por:

$$\ln L(\theta, \gamma)= n\ln(\gamma)-\gamma n\ln(\theta)+(\gamma-1)\sum_{i=1}^{n}\ln(x_{i})-\sum_{i=1}^{n}\left(\frac{x_{i}}{\theta}\right)^{\gamma}$$

i) Obtenga los EMV de $\gamma$ y $\theta$. Calcule su log-verosimilitud y grafique la f.d.a. empírica y la f.d.a. ajustada. ¿Qué puede concluir? [10]

```{r}
#Tambien se pudo usar de MASS  fitdistr(x, densfun, start, ...)
logveromisimilitud <- function(x, theta, gamma) {
  n <- length(x)
  resultado <- n * log(gamma) - gamma * n * log(theta) + (gamma - 1) * sum(log(x)) - sum((x / theta)^gamma)
  return(resultado)
}

neg_logverosimilitud <- function(params, x) {
  theta <- params[1]
  gamma <- params[2]
  -logveromisimilitud(x, theta, gamma)
}

# Valores iniciales para theta y gamma
valores_iniciales <- c(theta = 1, gamma = 1)

# Límites para theta y gamma
limites <- list(c(0.00001, 100), c(0.00001, 100))

# Utilizar optim con el método "L-BFGS-B" para encontrar los parámetros óptimos

resultado_optim <- optim(
  par = valores_iniciales,
  fn = neg_logverosimilitud,
  x = datos$Monto,
  method = "L-BFGS-B",
  lower = sapply(limites, `[`, 1), # Límites inferiores
  upper = sapply(limites, `[`, 2)  # Límites superiores
)

# Los parámetros optimizados

theta_estimado <- resultado_optim$par[1]
gamma_estimado <- resultado_optim$par[2]

logver1 <- logveromisimilitud(datos$Monto, theta_estimado, gamma_estimado)

```

Con lo cual los valores de $\hat{\theta}$ y $\hat{\gamma}$ son `r theta_estimado` y `r gamma_estimado` respectivamente. Además, el valor de la log-verosimilitud es `r logveromisimilitud(datos$Monto, theta_estimado, gamma_estimado)`. 

Ahora graficaremos la f.d.a. empírica y la f.d.a. ajustada.

```{r}
# Crear un nuevo dataframe con valores para las funciones de distribución acumulativa
n <- 100
valores <- seq(0, 4, length.out = n)
df_weibull <- data.frame(
  Monto = valores,
  FittedWeibull = pweibull(valores, shape = gamma_estimado, scale = theta_estimado)
)

# Crear la distribución empírica acumulada (ECDF)
df_empirica <- data.frame(
  Monto = valores,
  Empirical = ecdf(datos$Monto)(valores)
)

# Combinar ambos dataframes
df_combined <- merge(df_weibull, df_empirica, by = "Monto")

# Gráfico con ggplot2
ggplot() +
  geom_step(aes(x = Monto, y = Empirical), data = df_empirica, color = "blue") +
  geom_line(aes(x = Monto, y = FittedWeibull), data = df_weibull, color = "red") +
  labs(title = "Weibull vs. Empirical Cumulative Distribution",
       x = "Monto",
       y = "CDF") +
  theme_minimal() +
  scale_x_continuous(expand = c(0, 0)) + 
  scale_y_continuous(expand = c(0, 0))
```

Al parecer fue un buen ajuste pues la f.d.a. ajustada se parece mucho a la f.d.a. empírica. Pero cabe señalar que la f.d.a. ajustada sobreestima poco antes de 1.5 y mayor a 2.3 aproximadamente, y subestima en los otros casos.

ii) Para el ajuste del inciso anterior aplique las pruebas Kolmogorov-Smirnov y Anderson-Darling considerando un nivel de significancia del 5%. (Sugerencia: Utilice EViews). [5]

```{r}
# Realizar la prueba de Kolmogorov-Smirnov
ks_result <- ks.test(datos$Monto, "pweibull", shape = gamma_estimado, scale = theta_estimado)

print(ks_result)

# Realizar la prueba de Anderson-Darling
library(ADGofTest)
# Define una función de distribución acumulada para la Weibull con tus parámetros estimados
pweibull_with_params <- function(q) {
  pweibull(q, shape = gamma_estimado, scale = theta_estimado)
}

# Realiza la prueba de Anderson-Darling usando tu vector de datos y la función de distribución definida
ad_result <- ADGofTest::ad.test(datos$Monto, pweibull_with_params)

# Ver los resultados de la prueba
print(ad_result)
```

En las dos pruebas no se rechaza la hipótesis nula de que la distribución de los datos es Weibull. Por lo que fue un buen ajuste.

iii) Suponga que $\gamma = 3$ y que se aplica un deducible de 1.5 millones de dólares. Obtenga en este caso el EMV de $\theta$ y calcule su log-verosimilitud. [10]

En este caso solo notemos que ahora la función de verosimilitud está dada por:

$$L(\theta)=\frac{ \frac{3^{n}}{\theta^{3 n}}\left(\Pi_{i=1}^{n}x_{i}\right)^{2}e^{-\sum_{i=1}^{n}(\frac{x_{i}}{\theta})^{3}}}{ e^{(\frac{1.5}{\theta})^{3n}}}$$

Con lo cual la log-verosimilitud está dada por:

$$\ln L(\theta)= n\ln(3)-3 n\ln(\theta)+2\sum_{i=1}^{n}\ln(x_{i})-\sum_{i=1}^{n}\left(\frac{x_{i}}{\theta}\right)^{3}+n\left(\frac{1.5}{\theta}\right)^{3}$$

Considerando que ahora $n$ es el número de datos mayores al deducible, los demás los ignoramos.

```{r}
datosDeducible <- datos[datos$Monto >= 1.5,]

# logVerosimilitud

logveromisimilitud <- function(x, theta) {
  n <- length(x)
  resultado <- n * log(3) - 3 * n * log(theta) + (3 - 1) * sum(log(x)) - sum((x / theta)^3)+n*(1.5/theta)^3
  return(resultado)
}

neglogverosimilitud <- function(params, x) {
  theta <- params[1]
  -logveromisimilitud(x, theta)
}

# Valores iniciales para theta
valores_iniciales <- 1

# Límites para theta
limites <- list(c(0.00001, 100))

# Utilizar optim con el método "L-BFGS-B" para encontrar los parámetros óptimos

resultado_optim <- optim(
  par = valores_iniciales,
  fn = neglogverosimilitud,
  x = datosDeducible$Monto,
  method = "L-BFGS-B",
  lower = sapply(limites, `[`, 1), # Límites inferiores
  upper = sapply(limites, `[`, 2)  # Límites superiores
)

# Los parámetros optimizados

theta_estimado <- resultado_optim$par[1]
logver2 <- logveromisimilitud(datosDeducible$Monto, theta_estimado)
```

Por lo tanto, el valor de $\hat{\theta}$ es `r theta_estimado` y el valor de la log-verosimilitud es `r logveromisimilitud(datosDeducible$Monto, theta_estimado)`.


iv) Suponga que $\gamma = 3$ y que no se aplica deducible pero se establece un límite de aseguramiento de 2.5 millones de pesos. Obtenga en este caso el EMV de $\theta$ y calcule su log-verosimilitud. [10]

En este caso la función de verosimilitud está dada por:

$$L(\theta)=\frac{3^{m_1}}{\theta^{3 m_1}}\left(\Pi_{i=1}^{m_1}x_{i}\right)^{2}e^{-\sum_{i=1}^{m_1}(\frac{x_{i}}{\theta})^{3}}(e^{-\sum_{i=1}^{m_2}(\frac{2.5}{\theta})^{3}})$$

donde $m_1$ es el número de datos menores al límite de aseguramiento y $m_2$ es el número de datos mayores al límite de aseguramiento.

Y por lo tanto la log-verosimilitud está dada por:

$$\ln L(\theta)= m_1\ln(3)-3 m_1\ln(\theta)+2\sum_{i=1}^{m_1}\ln(x_{i})-\sum_{i=1}^{m_1}\left(\frac{x_{i}}{\theta}\right)^{3}-\sum_{i=1}^{m_2}\left(\frac{2.5}{\theta}\right)^{3}$$





```{r}

logveromisimilitud <- function(x, theta) {
  n <- length(x)
  xind <- x[x < 2.5]
  xout <- x[x >= 2.5]
  m1 <- length(xind)
  m2 <- length(xout)
  resultado <- m1 * log(3) - 3 * m1 * log(theta) + (3 - 1) * sum(log(xind)) - sum((xind / theta)^3)-m2*(2.5 / theta)^3
  return(resultado)
}

neglogverosimilitud <- function(params, x) {
  theta <- params[1]
  -logveromisimilitud(x, theta)
}

# Valores iniciales para theta
valores_iniciales <- 1

# Límites para theta
limites <- list(c(0.00001, 100))

# Utilizar optim con el método "L-BFGS-B" para encontrar los parámetros óptimos

resultado_optim <- optim(
  par = valores_iniciales,
  fn = neglogverosimilitud,
  x = datos$Monto,
  method = "L-BFGS-B",
  lower = sapply(limites, `[`, 1), # Límites inferiores
  upper = sapply(limites, `[`, 2)  # Límites superiores
)

# Los parámetros optimizados

theta_estimado <- resultado_optim$par[1]
logver3 <- logveromisimilitud(datos$Monto, theta_estimado)

```

Con lo cual el valor de $\hat{\theta}$ es `r theta_estimado` y el valor de la log-verosimilitud es `r logveromisimilitud(datos$Monto, theta_estimado)`.

v) Calcule los Criterios de Información de Akaike, Schwartz y Hannan-Quinn para los modelos ajustados en los subincisos i), iii) y iv). ¿Cuál modelo se prefiere? Justifique brevemente su respuesta. [5]

```{r}
# Criterios
akaike_criterio <- function(logverosimilitud, k, n) {
  resultado <- (-2 * logverosimilitud + 2 * k)/n
  return(resultado)
}

schwartz_criterio <- function(logverosimilitud, k, n) {
  resultado <- (-2 * logverosimilitud + 2 * log(n))/n
  return(resultado)
}

hannan_quinn_criterio <- function(logverosimilitud, k, n) {
  resultado <- (-2 * logverosimilitud + 2 * k * log(log(n)))/n
  return(resultado)
}


```



|                      | (i)                  | (ii)                 | (iii)                |
|----------------------|----------------------|----------------------|----------------------|
| n          | `r length(datos$Monto)`| `r length(datosDeducible$Monto)`| `r length(datos$Monto)`|
| Log-verosimilitud    | `r logver1`         | `r logver2`        | `r logver3`       |
| q                    | `r 2`              | `r 1`             | `r 1`            |
| **Criterios de información** |                      |                      |                      |
| Akaike | `r akaike_criterio(logver1,2,length(datos$Monto))` | `r akaike_criterio(logver2,1,length(datosDeducible$Monto))`| `r akaike_criterio(logver3,1,length(datos$Monto))`|
| Schwartz             | `r schwartz_criterio(logver1,2,length(datos$Monto))`            | `r schwartz_criterio(logver2,1,length(datosDeducible$Monto))`           | `r schwartz_criterio(logver3,1,length(datos$Monto))`          |
| Hannan-Quinn         | `r hannan_quinn_criterio(logver1,2,length(datos$Monto))`            | `r hannan_quinn_criterio(logver2,1,length(datosDeducible$Monto))`           | `r hannan_quinn_criterio(logver3,1,length(datos$Monto))`          |


Notamos que el modelo que tiene menor valor en los 3 criterios de información es el modelo del inciso iii). Por lo que es el modelo que más se preferirá y el que mejor ajustó a los datos siguiendo cierta parsimonía con el número de parámetros que se estimaron. Es decir, el modelo con límite de aseguramiento (sin tomar en cuenta deducible) es mejor para poder ajustar los datos a una Weibull.

# Ejercicio 11

# Aproximación Binomial Negativa por Poisson

Suponga que 
$$
Y \sim \text{Po}(\lambda), \quad \lambda > 0;
$$

$$
N_i \sim \text{iid Geo}(p), \quad 0 < p < 1;
$$ 

y que

$$
N = N_1 + N_2 + \ldots + N_r, \quad r \in \mathbb{Z}^+.
$$

d) Para $N \sim \text{BN}(50, 0.2)$ se quieren obtener $P(N = 0)$ y $P(\left|N - E[N]\right| \leq \sigma)$.

i) Calcule exactamente ambas probabilidades. [5]

$$
P(N = 0)
$$

$$
P(\left|N - E[N]\right| \leq \sigma)
$$

ii) Aproxime ambas probabilidades mediante la distribución Poisson. ¿Cuál es el error de la aproximación? [5]

iii) Aproxime ambas probabilidades mediante el Teorema Central del Límite aplicando el Ajuste de Yate. ¿Cuál es el error de aproximación? ¿Qué puede concluir? [10]


```{r}

```


# Ejercicio 12 **Clase (a, b, 0) vs. Clase (a, b, 1).**

Suponga que $N \sim \text{Bin}(m, q)$, con $m \in \mathbb{Z}^+$ y $0 < q < 1$.

a) Demuestre que $N$ pertenece a la Clase $(a, b, 0)$ y especifique los valores de los parámetros $a$ y $b$, así como de $P(N = 0)$. [10]

b) Obtenga la función de masa de probabilidad de la distribución Binomial truncada en cero y verifique analíticamente que pertenece a la Clase $(a, b, 1)$. [10]

c) Si $m = 20$ y $q = 0.3$, grafique la función de masa de probabilidad de $N$ junto con la función de masa de probabilidad de su respectiva distribución Binomial modificada en cero considerando $P(N = 0) = 0.2$. Verifique numéricamente que la Binomial modificada en cero pertenece a la Clase $(a, b, 1)$. [10]


```{r}

```