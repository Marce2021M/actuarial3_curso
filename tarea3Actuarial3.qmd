---
title: "Actuarial 3 Tarea 3"
subtitle: "Actuarial 3"
lang: es
author: "Marcelino Sánchez"
date: today
format:
  html:
    page-layout: full
    embed-resources: true
editor_options: 
  chunk_output_type: console
---

# Ejercicio 2: Empalmes

Para un seguro de daños, se conoce que las pérdidas menores a $c$ siguen una distribución $Exp(\frac{1}{\theta})$ con $\theta > 0$, y las pérdidas mayores o iguales a $c$ tienen una distribución $Pareto(\alpha, \beta)$ donde $\alpha > 0$ y $\beta >0$.

a) Se pide demostrar que la función de densidad de la Distribución de Empalme está dada por:

$f_X(x) = p \frac{\theta^{-1}e^{-\frac{x}{\theta}}}{1-e^{-\frac{c}{\theta}}} I(0 < x < c) + (1-p) \frac{\alpha (c+\beta)^{\alpha}}{(x+\beta)^{\alpha + 1}} I(x \geq c)$

Dado:

- $\theta = 100$
- $\alpha = 4$
- $\beta = 200$
- $c = 100$

Se pide:

i) Graficar $f_X(x)$ con $p = 0.6$. Determinar si $f_X(x)$ es continua en $x = 100$.

```{r, echo=FALSE, message=FALSE, warning=FALSE}
# Instala e importa las librerías necesarias
library(ggplot2)

# Parámetros dados
p <- 0.6
theta <- 100  
alpha <- 4  
beta <- 200   
c <- 100      

# Definición de la función f_X(x)
f_X <- function(x) {
  if(x < c & x > 0){
    resultado = p * (theta^(-1) * exp(-x/theta))/(1 - exp(-c/theta))}
  else if (x >= c ){
    resultado = (1-p) * (alpha * (c+beta)^alpha)/((x + beta)^(alpha+1))
  }else{
    resultado = 0
  }
  return(resultado)
}

# Genera valores para x y f_X(x)
x_vals <- seq(.001, c-.0001, length.out = 1000)  # Modifica el rango de x si es necesario
y_vals <- sapply(x_vals, f_X)

# Grafica la función
df1 <- data.frame(x = x_vals, y = y_vals)

x_vals <- seq(c+.001, 300, length.out = 1000)  # Modifica el rango de x si es necesario
y_vals <- sapply(x_vals, f_X)

df2 <- data.frame(x = x_vals, y = y_vals)

x_vals <- seq(-50, -.001, length.out = 1000)  # Modifica el rango de x si es necesario
y_vals <- sapply(x_vals, f_X)

df3 <- data.frame(x = x_vals, y = y_vals)
df1$y[df1$x == c] <- NA
df2$y[df2$x == c] <- NA
df3$y[df3$x == c] <- NA

ggplot() + 
  geom_line(data=df1, aes(x, y)) +
  geom_line(data=df2, aes(x, y)) +
  geom_line(data=df3, aes(x, y)) +
  geom_point(aes(x = 0, y = f_X(0)), color = "red", size = 3 ) +
  geom_point(aes(x = c, y = f_X(c)), color = "red", size = 3) +
  ggtitle(expression(f[X](x))) +
  geom_point(aes(x = 0, y = f_X(0.001)), color = "red", size = 3 , shape=1) +
  geom_point(aes(x = c, y = f_X(c-.001)), color = "red", size = 3, shape=1) +
  ggtitle(expression(f[X](x))) +
  theme_minimal()

```

ii) Encontrar el valor de $ p $ para que $ f_X(x) $ sea continua en $ x = 100 $ y luego graficar $ f_X(x) $ con este valor.

```{r}
theta <- 100  
alpha <- 4  
beta <- 200   
c <- 100 
aux1 <- ((c+beta)*(theta^(-1))*((exp(-c/theta))/(1-exp(-c/theta)))
 + alpha)
pvalor <- alpha/aux1
```

El valor de p en el cual se satisface la continuidad en c es `r pvalor`

```{r, echo=FALSE, message=FALSE, warning=FALSE}
p <- pvalor
# Genera valores para x y f_X(x)
x_vals <- seq(.001, c-.0001, length.out = 1000)  # Modifica el rango de x si es necesario
y_vals <- sapply(x_vals, f_X)

# Grafica la función
df1 <- data.frame(x = x_vals, y = y_vals)

x_vals <- seq(c+.001, 300, length.out = 1000)  # Modifica el rango de x si es necesario
y_vals <- sapply(x_vals, f_X)

df2 <- data.frame(x = x_vals, y = y_vals)

x_vals <- seq(-50, -.001, length.out = 1000)  # Modifica el rango de x si es necesario
y_vals <- sapply(x_vals, f_X)

df3 <- data.frame(x = x_vals, y = y_vals)
df1$y[df1$x == c] <- NA
df2$y[df2$x == c] <- NA
df3$y[df3$x == c] <- NA

ggplot() + 
  geom_line(data=df1, aes(x, y)) +
  geom_line(data=df2, aes(x, y)) +
  geom_line(data=df3, aes(x, y)) +
  geom_point(aes(x = 0, y = f_X(0)), color = "red", size = 3 ) +
  geom_point(aes(x = c, y = f_X(c)), color = "red", size = 3) +
  ggtitle(expression(f[X](x))) +
  geom_point(aes(x = 0, y = f_X(0.001)), color = "red", size = 3 , shape=1) +
  geom_point(aes(x = c, y = f_X(c-.001)), color = "red", size = 3, shape=1) +
  ggtitle(expression(f[X](x))) +
  theme_minimal()

```



# Ejercicio 4: Colas de las distribuciones

Suponga que $X \sim \text{Pareto}(\alpha, \theta)$ y  $Y \sim \text{Gamma}(\tau, \beta)$.


## Comparación de distribuciones

b) Para comparar de manera más justa ambas distribuciones, queremos que ambas tengan media 5 y varianza 75.

i) Encuentre los valores de $\alpha$, $\theta$, $\tau$, y $\beta$.

```{r}

#Calculamos los parámetros de la pareto
varianza <- 75

esperanza <- 5

aux1 <- varianza/(esperanza^2) +1

aux2 <- varianza/(esperanza^3) -1/esperanza

theta <- aux1/aux2

alpha <- theta/esperanza + 1

#Calculamos los parámetros de la gamma

tau <- (esperanza^2)/varianza

beta <- varianza/esperanza

```

Los valores de los parámetros son:

- $\alpha = `r alpha`$
- $\theta = `r theta`$
- $\tau = `r tau`$
- $\beta = `r beta`$


ii) Compare en la misma gráfica ambas funciones de densidad. ¿Qué conclusiones se pueden derivar?

```{r, echo=FALSE, message=FALSE, warning=FALSE}
library(actuar)

library(ggplot2)

# Define parameters for the Pareto distribution
b <- alpha
scale_pareto <- theta

# Define parameters for the Gamma distribution
a <- tau
scale_gamma <- beta

# Create x values
x <- seq(0, 5, length.out = 1000)

# Compute PDFs
pdf_pareto <- dpareto(x, shape = b, scale = scale_pareto)
pdf_gamma <- dgamma(x, shape = a, scale = scale_gamma)

# Create data frame for ggplot
df <- data.frame(x, pdf_pareto, pdf_gamma)

# Plot both distributions on the same graph using ggplot
ggplot(df, aes(x)) + 
  geom_line(aes(y = pdf_pareto, color = paste("Pareto b=", b))) +
  geom_line(aes(y = pdf_gamma, color = paste("Gamma a=", a, ", scale=", scale_gamma))) +
  labs(title = "Pareto vs Gamma Distributions", y = "Density", color = "Distribution") +
  theme_minimal() +
  coord_cartesian(ylim = c(0, 0.5))  # set y-axis limit here

```


Se puede derivar que la gamma acumula más que la pareto para valores cercanos a cero (aún teniendo la misma varianza y media), por lo que la pareto tiene cola más pesada que la gamma.

# Ejercicio 5

Verosimilitud para datos agrupados. Suponga que las pérdidas de cierto riesgo siguen una distribución con función de distribución acumulada $F(x) = (1 - \frac{\theta}{x})I(\theta, \infty)(x)$, donde $\theta > 0$. Una muestra aleatoria de 20 siniestros contiene 9 pérdidas por debajo de 10 mil dólares, 6 pérdidas entre 10 y 25 mil dólares y 5 pérdidas que exceden los 25 mil dólares. Encuentre el estimador de máxima verosimilitud de $\theta$. [10]

Por principio de máxima verosimilitud tenemos que el logaritmo de la función de verosimilitud para datos agrupados está dada por:

$$\ln(L(\theta)) = 9 \ln\left( F(10000, \theta) - F(0, \theta) \right) + 6 \ln\left( F(25000, \theta) - F(10000, \theta) \right)$$

$$+5\ln\left( 1 - F(25000, \theta)\right)$$


```{r}

acumulada <- function(x, theta){
  if(x > theta){
    resultado = 1 - (theta/x)
  }else{
    resultado = 0
  }
  return(resultado)
}


verosimilitud <- function(theta){
  resultado <- 9*log(acumulada(10000, theta)-acumulada(0,theta)) + 6*log(acumulada(25000, theta)-acumulada(10000,theta)) + 5*log(1-acumulada(25000,theta))
  return(resultado)
}

neg_verosimilitud <- function(theta) {
  return(-verosimilitud(theta))
}

# Usamos la función optimize para encontrar el valor de theta que maximiza la verosimilitud
# Establece un rango razonable para theta basado en tu conocimiento del problema
resultado <- optimize(neg_verosimilitud, interval=c(0, 9000))

```

El valor de $\theta$ que maximiza la verosimilitud numéricamente es `r resultado$minimum` .

# Ejercicio 6

Ajuste para datos agrupados. A continuación se presenta la distribución de frecuencias de una muestra de 250 pérdidas de un seguro de hogar.

| Pérdida (miles de pesos) | Frecuencia |
|--------------------------|------------|
| 0 - 50                   | 3          |
| 50 - 150                 | 54         |
| 150 - 300                | 62         |
| 300 - 500                | 43         |
| 500 - 1,000              | 39         |
| 1,000 - 2,000            | 24         |
| 2,000 - 4,000            | 11         |
| 4,000 - 8,000            | 9          |
| 8,000 - 16,000           | 4          |
| Más de 16,000            | 1          |
| **Total**                | **250**    |

a) Construya el histograma de frecuencias relativas respectivo. Note que las clases son de distinta longitud. [10]

```{r}
# Crea un vector con los límites de las clases

clases <- c(0, 50, 150, 300, 500, 1000, 2000, 4000, 8000, 16000, 24000)

# Crea un vector con las frecuencias

frecuencias <- c(3, 54, 62, 43, 39, 24, 11, 9, 4, 1)

# Crea un vector con las frecuencias relativas

frecuencias_relativas <- frecuencias/sum(frecuencias)

# Crea un vector con las longitudes de las clases

longitudes_clases <- diff(clases)


# Crea un vector con los factores de ajuste

factores_ajuste <- 1/longitudes_clases

# Crea un vector con las frecuencias relativas ajustadas

frecuencias_relativas_ajustadas <- frecuencias_relativas * factores_ajuste

# Crea un vector con las frecuencias relativas corregidas

frecuencias_relativas_corregidas <- frecuencias_relativas_ajustadas/sum(frecuencias_relativas_ajustadas)

# Crea un vector con los puntos medios de las clases

puntos_medios_clases <- (clases[-1] + clases[-length(clases)])/2

# Crea un data frame con los datos

df <- data.frame( frecuencias, frecuencias_relativas, frecuencias_relativas_ajustadas, frecuencias_relativas_corregidas, puntos_medios_clases)


# Grafica el histograma
gg1 <-ggplot(df, aes(x = puntos_medios_clases, y = frecuencias_relativas_corregidas)) +
  geom_bar(width = longitudes_clases, stat = "identity", fill = "steelblue") +
  geom_text(aes(label = frecuencias), vjust = -0.5) +
  ggtitle("Histograma de frecuencias relativas") +
  xlab("Pérdida (miles de pesos)") +
  ylab("Frecuencia relativa") +
  theme_minimal()

gg1

ggplot(df[1:6, ], aes(x = puntos_medios_clases, y = frecuencias_relativas_corregidas)) +
  geom_bar(width = longitudes_clases[1:6], stat = "identity", fill = "steelblue") +
  geom_text(aes(label = round(frecuencias_relativas_corregidas,digits = 5)), vjust = -0.5) +
  ggtitle("Histograma de frecuencias relativas (primeras 8 clases)") +
  xlab("Pérdida (miles de pesos)") +
  ylab("Frecuencia relativa") +
  xlim(c(0, 2000)) +  # Limitar el eje x hasta un poco después de 8,000
  theme_minimal()





```



b) A partir de los datos agrupados, ajuste por máxima verosimilitud la Distribución Exponencial Inversa con función de distribución acumulada 
$F(x) = e^{-\frac{\theta}{x}} I(0,\infty)(x), \quad \theta > 0$.
Especifique $\hat{\theta}$ y grafique la densidad ajustada junto con el histograma del inciso anterior. [10]


Por principio de máxima verosiimilitud tenemos que el logaritmo de la función de verosimilitud para datos agrupados está dada por:

$$\ln(L(\theta)) = \sum_{i=1}^{10} n_i \ln\left( F(b_i, \theta) - F(a_i, \theta) \right)$$

```{r}
acumulada <- function(x, theta){
  if(x > 0){
    resultado = exp(-theta/x)
  }else{
    resultado = 0
  }
  return(resultado)
}


verosimilitud <- function(theta){
  for(i in 1:10){
    if(i == 1){
      resultado <- frecuencias[i]*log(acumulada(clases[i+1], theta)-acumulada(clases[i],theta))
    }else{
      resultado <- resultado + frecuencias[i]*log(acumulada(clases[i+1], theta)-acumulada(clases[i],theta))
    }
  }
  return(resultado)
}

neg_verosimilitud <- function(theta) {
  return(-verosimilitud(theta))
}

# Usamos la función optimize para encontrar el valor de theta que maximiza la verosimilitud
# Establece un rango razonable para theta basado en tu conocimiento del problema
resultado <- optimize(neg_verosimilitud, interval=c(0, 9000))



# Compute likelihoods

d_gamma_inv <- function(x, theta) {
  ifelse(x > 0, (theta / (x^(2))) * exp(-theta/x),0)
}

# Valores de x
x_vals <- seq(0, 24000, by = 100)

# Calcular la función de densidad para estos valores
y_vals <- sapply(x_vals, d_gamma_inv, theta =resultado$minimum )

# Graficar

data_to_plot2 <- data.frame(x = x_vals, y = y_vals)

ggplot(data_to_plot2, aes(x = x, y = y)) +
  geom_line(color = "blue") +
  ggtitle(paste("Distribución Gama Inversa: alpha =", 1, ", beta =", resultado$minimum)) +
  xlab("x") +
  ylab("f(x)") +
  theme_minimal() 

# Primero, configura el ggplot base
plot_base <- ggplot() + 
  theme_minimal() +
  xlab("x") + ylab("f(x)")

# Supongo que plot_base y los otros datasets ya están definidos

# Calcula el factor de transformación. Este factor se basará en la relación entre los rangos máximos de tus datos.
max_y_data_to_plot2 <- max(data_to_plot2$y)
max_df <- max(df$frecuencias_relativas_corregidas)

# Factor de transformación
trans_factor <- max_y_data_to_plot2 / max_df

final_plot <- plot_base +
  # Primero traza el histograma
  geom_bar(data = df, aes(x = puntos_medios_clases, y = frecuencias_relativas_corregidas, width = longitudes_clases), stat = "identity", fill = "steelblue") +
  # Luego traza la línea, pero multiplica los valores por el factor de transformación
  geom_line(data = data_to_plot2, aes(x = x, y = y / trans_factor), color = "red") +
  ggtitle(paste("Distribución Gama Inversa: alpha =", 1, ", beta =", resultado$minimum)) +
  # Añade el eje secundario, dividiendo por el factor de transformación para mostrar los valores reales
  scale_y_continuous(sec.axis = sec_axis(~./trans_factor, name = "Eje secundario"))

# Finalmente, muestra el gráfico
print(final_plot)

```

El valor de $\theta$ que maximiza la verosimilitud numéricamente es `r resultado$minimum` .

# Ejercicio 7

Ajuste de una mezcla. Con base en 180 pérdidas de un seguro de vehículos que cubre
automóviles y motocicletas se construyó el siguiente histograma:

```{r}
library(readxl)
library(kableExtra)
ruta <- paste0(getwd(), "/bases/Tarea 3.xlsx")
datosPerdida <- read_excel(ruta, sheet = "7", range = "A8:B188")

hist(datosPerdida$Pérdida)
```

Se quiere ajustar una mezcla de distribuciones Ji-Cuadradas con medias $\nu$ y $2\nu$ con ponderadores $\alpha$ y $1-\alpha$, respectivamente, donde $\nu \in \mathbb{Z^+}$ y $0 < \alpha < 1$.

**a)** A partir de los datos individuales, escriba la función de densidad de la mezcla y verifique que los EMV son $\hat{\alpha} = 0.2468$ y $\hat{\nu} = 30$. (Sugerencia: Optimice numéricamente respecto a ambos parámetros, luego fije $\hat{\nu}$ al entero más cercano y maximice nuevamente para obtener $\hat{\alpha}$). [15]

Primero notamos que la función de verosimilitud está dada por:

$$
L(\nu, \alpha)= \prod_{i=1}^{180} \left[ \alpha \frac{x^{\frac{\nu}{2}-1} e^{-\frac{x}{2}}}{2^{\frac{\nu}{2}} \Gamma\left(\frac{\nu}{2}\right)} + (1-\alpha)\frac{x^{\nu-1} e^{-\frac{x}{2}}}{2^{\nu} \Gamma(\nu)} \right]
$$

Por lo que el logaritmo de la función de verosimilitud está dado por:

$$
\ln(L(\nu, \alpha)) = \sum_{i=1}^{180} \left[ \ln\left( \alpha \frac{x^{\frac{\nu}{2}-1} e^{-\frac{x}{2}}}{2^{\frac{\nu}{2}} \Gamma\left(\frac{\nu}{2}\right)} + (1-\alpha)\frac{x^{\nu-1} e^{-\frac{x}{2}}}{2^{\nu} \Gamma(\nu)} \right) \right]
$$



```{r}
mezclaFuncion <- function(x, alpha , nu){
  resultado <- alpha*dgamma(x, shape = nu/2, scale = 2) + (1-alpha)*dgamma(x, shape = nu, scale = 2)
  return(resultado)
}

verosimilitud <- function(alpha,nu){
  for(i in 1:180){
    if(i == 1){
      resultado <- log(mezclaFuncion(datosPerdida$Pérdida[i], alpha, nu))
    }else{
      resultado <- resultado + log(mezclaFuncion(datosPerdida$Pérdida[i], alpha, nu))
    }
  }
  return(resultado)
}

neg_verosimilitud <- function(theta) {
  return(-verosimilitud(theta))
}

# Usamos la función optimize para encontrar el valor de theta que maximiza la verosimilitud
# Establece un rango razonable para theta basado en tu conocimiento del problema




# Compute likelihoods

d_gamma_inv <- function(x, theta) {
  ifelse(x > 0, (theta / (x^(2))) * exp(-theta/x),0)
}

# Valores de x
x_vals <- seq(0, 24000, by = 100)

# Calcular la función de densidad para estos valores
y_vals <- sapply(x_vals, d_gamma_inv, theta =resultado$minimum )

# Graficar

data_to_plot2 <- data.frame(x = x_vals, y = y_vals)

ggplot(data_to_plot2, aes(x = x, y = y)) +
  geom_line(color = "blue") +
  ggtitle(paste("Distribución Gama Inversa: alpha =", 1, ", beta =", resultado$minimum)) +
  xlab("x") +
  ylab("f(x)") +
  theme_minimal() 

# Primero, configura el ggplot base
plot_base <- ggplot() + 
  theme_minimal() +
  xlab("x") + ylab("f(x)")

# Supongo que plot_base y los otros datasets ya están definidos

# Calcula el factor de transformación. Este factor se basará en la relación entre los rangos máximos de tus datos.
max_y_data_to_plot2 <- max(data_to_plot2$y)
max_df <- max(df$frecuencias_relativas_corregidas)

# Factor de transformación
trans_factor <- max_y_data_to_plot2 / max_df

final_plot <- plot_base +
  # Primero traza el histograma
  geom_bar(data = df, aes(x = puntos_medios_clases, y = frecuencias_relativas_corregidas, width = longitudes_clases), stat = "identity", fill = "steelblue") +
  # Luego traza la línea, pero multiplica los valores por el factor de transformación
  geom_line(data = data_to_plot2, aes(x = x, y = y / trans_factor), color = "red") +
  ggtitle(paste("Distribución Gama Inversa: alpha =", 1, ", beta =", resultado$minimum)) +
  # Añade el eje secundario, dividiendo por el factor de transformación para mostrar los valores reales
  scale_y_continuous(sec.axis = sec_axis(~./trans_factor, name = "Eje secundario"))

# Finalmente, muestra el gráfico
print(final_plot)



```

**b)** Grafique la función de densidad ajustada junto con el histograma de frecuencias relativas y obtenga los EMV de las modas. [15]

# Ejercicio 8

Pruebas de bondad de ajuste. A continuación se muestran los montos (en miles de
dólares) de 60 siniestros correspondientes a una póliza de incendio:

**a)** Construya un histograma de frecuencias relativas con 6 clases de igual longitud entre $30,000$ y $90,000$ dólares. ¿Qué distribuciones paramétricas podrían ajustar a este perfil de siniestros? [10]

**b)** Si se decide ajustar una Distribución Lognormal con parámetros $\mu \in \mathbb{R}$ y $\sigma^2 > 0$, calcule sus respectivos EMV. Grafique la función de distribución acumulada ajustada junto con la Ojiva. ¿Qué puede concluir? (Sugerencia: Recuerde que estos estimadores fueron deducidos en el Ejercicio E19). [10]

**c)** Construya la Gráfica de Probabilidad para el ajuste del inciso b). ¿Qué puede concluir? [10]

**d)** Aplique la Prueba Kolmogorov-Smirnov para validar el ajuste Lognormal del inciso b). Considere un tamaño de error tipo I del 5%. [10]

**e)** Aplique la Prueba Ji-Cuadrada para validar el ajuste Lognormal del inciso b). Considere la partición del inciso a) y agregue 2 clases (inicial y final) para abarcar todo el soporte de la Lognormal. Calcule el valor-p para concluir. [10]

