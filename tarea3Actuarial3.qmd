---
title: "Actuarial 3 Tarea 3"
subtitle: "Actuarial 3"
lang: es
author: "Marcelino Sánchez"
date: today
format:
  html:
    page-layout: full
    embed-resources: true
editor_options: 
  chunk_output_type: console
---

# Ejercicio 2: Empalmes

Para un seguro de daños, se conoce que las pérdidas menores a $c$ siguen una distribución $Exp(\frac{1}{\theta})$ con $\theta > 0$, y las pérdidas mayores o iguales a $c$ tienen una distribución $Pareto(\alpha, \beta)$ donde $\alpha > 0$ y $\beta >0$.

a) Se pide demostrar que la función de densidad de la Distribución de Empalme está dada por:

$f_X(x) = p \frac{\theta^{-1}e^{-\frac{x}{\theta}}}{1-e^{-\frac{c}{\theta}}} I(0 < x < c) + (1-p) \frac{\alpha (c+\beta)^{\alpha}}{(x+\beta)^{\alpha + 1}} I(x \geq c)$

Dado:

- $\theta = 100$
- $\alpha = 4$
- $\beta = 200$
- $c = 100$

Se pide:

i) Graficar $f_X(x)$ con $p = 0.6$. Determinar si $f_X(x)$ es continua en $x = 100$.

```{r, echo=FALSE, message=FALSE, warning=FALSE}
# Instala e importa las librerías necesarias
library(ggplot2)

# Parámetros dados
p <- 0.6
theta <- 100  
alpha <- 4  
beta <- 200   
c <- 100      

# Definición de la función f_X(x)
f_X <- function(x) {
  if(x < c & x > 0){
    resultado = p * (theta^(-1) * exp(-x/theta))/(1 - exp(-c/theta))}
  else if (x >= c ){
    resultado = (1-p) * (alpha * (c+beta)^alpha)/((x + beta)^(alpha+1))
  }else{
    resultado = 0
  }
  return(resultado)
}

# Genera valores para x y f_X(x)
x_vals <- seq(.001, c-.0001, length.out = 1000)  # Modifica el rango de x si es necesario
y_vals <- sapply(x_vals, f_X)

# Grafica la función
df1 <- data.frame(x = x_vals, y = y_vals)

x_vals <- seq(c+.001, 300, length.out = 1000)  # Modifica el rango de x si es necesario
y_vals <- sapply(x_vals, f_X)

df2 <- data.frame(x = x_vals, y = y_vals)

x_vals <- seq(-50, -.001, length.out = 1000)  # Modifica el rango de x si es necesario
y_vals <- sapply(x_vals, f_X)

df3 <- data.frame(x = x_vals, y = y_vals)
df1$y[df1$x == c] <- NA
df2$y[df2$x == c] <- NA
df3$y[df3$x == c] <- NA

ggplot() + 
  geom_line(data=df1, aes(x, y)) +
  geom_line(data=df2, aes(x, y)) +
  geom_line(data=df3, aes(x, y)) +
  geom_point(aes(x = 0, y = f_X(0)), color = "red", size = 3 ) +
  geom_point(aes(x = c, y = f_X(c)), color = "red", size = 3) +
  ggtitle(expression(f[X](x))) +
  geom_point(aes(x = 0, y = f_X(0.001)), color = "red", size = 3 , shape=1) +
  geom_point(aes(x = c, y = f_X(c-.001)), color = "red", size = 3, shape=1) +
  ggtitle(expression(f[X](x))) +
  theme_minimal()

```

ii) Encontrar el valor de $ p $ para que $ f_X(x) $ sea continua en $ x = 100 $ y luego graficar $ f_X(x) $ con este valor.

```{r}
theta <- 100  
alpha <- 4  
beta <- 200   
c <- 100 
aux1 <- ((c+beta)*(theta^(-1))*((exp(-c/theta))/(1-exp(-c/theta)))
 + alpha)
pvalor <- alpha/aux1
```

El valor de p en el cual se satisface la continuidad en c es `r pvalor`

```{r, echo=FALSE, message=FALSE, warning=FALSE}
p <- pvalor
# Genera valores para x y f_X(x)
x_vals <- seq(.001, c-.0001, length.out = 1000)  # Modifica el rango de x si es necesario
y_vals <- sapply(x_vals, f_X)

# Grafica la función
df1 <- data.frame(x = x_vals, y = y_vals)

x_vals <- seq(c+.001, 300, length.out = 1000)  # Modifica el rango de x si es necesario
y_vals <- sapply(x_vals, f_X)

df2 <- data.frame(x = x_vals, y = y_vals)

x_vals <- seq(-50, -.001, length.out = 1000)  # Modifica el rango de x si es necesario
y_vals <- sapply(x_vals, f_X)

df3 <- data.frame(x = x_vals, y = y_vals)
df1$y[df1$x == c] <- NA
df2$y[df2$x == c] <- NA
df3$y[df3$x == c] <- NA

ggplot() + 
  geom_line(data=df1, aes(x, y)) +
  geom_line(data=df2, aes(x, y)) +
  geom_line(data=df3, aes(x, y)) +
  geom_point(aes(x = 0, y = f_X(0)), color = "red", size = 3 ) +
  geom_point(aes(x = c, y = f_X(c)), color = "red", size = 3) +
  ggtitle(expression(f[X](x))) +
  geom_point(aes(x = 0, y = f_X(0.001)), color = "red", size = 3 , shape=1) +
  geom_point(aes(x = c, y = f_X(c-.001)), color = "red", size = 3, shape=1) +
  ggtitle(expression(f[X](x))) +
  theme_minimal()

```



# Ejercicio 4: Colas de las distribuciones

Suponga que $X \sim \text{Pareto}(\alpha, \theta)$ y  $Y \sim \text{Gamma}(\tau, \beta)$.


## Comparación de distribuciones

b) Para comparar de manera más justa ambas distribuciones, queremos que ambas tengan media 5 y varianza 75.

i) Encuentre los valores de $\alpha$, $\theta$, $\tau$, y $\beta$.

```{r}

#Calculamos los parámetros de la pareto
varianza <- 75

esperanza <- 5

aux1 <- varianza/(esperanza^2) +1

aux2 <- varianza/(esperanza^3) -1/esperanza

theta <- aux1/aux2

alpha <- theta/esperanza + 1

#Calculamos los parámetros de la gamma

tau <- (esperanza^2)/varianza

beta <- varianza/esperanza

```

Los valores de los parámetros son:

- $\alpha = `r alpha`$
- $\theta = `r theta`$
- $\tau = `r tau`$
- $\beta = `r beta`$


ii) Compare en la misma gráfica ambas funciones de densidad. ¿Qué conclusiones se pueden derivar?

```{r, echo=FALSE, message=FALSE, warning=FALSE}
library(actuar)

library(ggplot2)

# Define parameters for the Pareto distribution
b <- alpha
scale_pareto <- theta

# Define parameters for the Gamma distribution
a <- tau
scale_gamma <- beta

# Create x values
x <- seq(0, 5, length.out = 1000)

# Compute PDFs
pdf_pareto <- dpareto(x, shape = b, scale = scale_pareto)
pdf_gamma <- dgamma(x, shape = a, scale = scale_gamma)

# Create data frame for ggplot
df <- data.frame(x, pdf_pareto, pdf_gamma)

# Plot both distributions on the same graph using ggplot
ggplot(df, aes(x)) + 
  geom_line(aes(y = pdf_pareto, color = paste("Pareto b=", b))) +
  geom_line(aes(y = pdf_gamma, color = paste("Gamma a=", a, ", scale=", scale_gamma))) +
  labs(title = "Pareto vs Gamma Distributions", y = "Density", color = "Distribution") +
  theme_minimal() +
  coord_cartesian(ylim = c(0, 0.5))  # set y-axis limit here

```


Se puede derivar que la gamma acumula más que la pareto para valores cercanos a cero (aún teniendo la misma varianza y media), por lo que la pareto tiene cola más pesada que la gamma.

# Ejercicio 5

Verosimilitud para datos agrupados. Suponga que las pérdidas de cierto riesgo siguen una distribución con función de distribución acumulada $F(x) = (1 - \frac{\theta}{x})I(\theta, \infty)(x)$, donde $\theta > 0$. Una muestra aleatoria de 20 siniestros contiene 9 pérdidas por debajo de 10 mil dólares, 6 pérdidas entre 10 y 25 mil dólares y 5 pérdidas que exceden los 25 mil dólares. Encuentre el estimador de máxima verosimilitud de $\theta$. [10]

Por principio de máxima verosimilitud tenemos que el logaritmo de la función de verosimilitud para datos agrupados está dada por:

$$\ln(L(\theta)) = 9 \ln\left( F(10000, \theta) - F(0, \theta) \right) + 6 \ln\left( F(25000, \theta) - F(10000, \theta) \right)$$

$$+5\ln\left( 1 - F(25000, \theta)\right)$$


```{r}

acumulada <- function(x, theta){
  if(x > theta){
    resultado = 1 - (theta/x)
  }else{
    resultado = 0
  }
  return(resultado)
}


verosimilitud <- function(theta){
  resultado <- 9*log(acumulada(10000, theta)-acumulada(0,theta)) + 6*log(acumulada(25000, theta)-acumulada(10000,theta)) + 5*log(1-acumulada(25000,theta))
  return(resultado)
}

neg_verosimilitud <- function(theta) {
  return(-verosimilitud(theta))
}

# Usamos la función optimize para encontrar el valor de theta que maximiza la verosimilitud
# Establece un rango razonable para theta basado en tu conocimiento del problema
resultado <- optimize(neg_verosimilitud, interval=c(0, 9000))

```

El valor de $\theta$ que maximiza la verosimilitud numéricamente es `r resultado$minimum` .

# Ejercicio 6

Ajuste para datos agrupados. A continuación se presenta la distribución de frecuencias de una muestra de 250 pérdidas de un seguro de hogar.

| Pérdida (miles de pesos) | Frecuencia |
|--------------------------|------------|
| 0 - 50                   | 3          |
| 50 - 150                 | 54         |
| 150 - 300                | 62         |
| 300 - 500                | 43         |
| 500 - 1,000              | 39         |
| 1,000 - 2,000            | 24         |
| 2,000 - 4,000            | 11         |
| 4,000 - 8,000            | 9          |
| 8,000 - 16,000           | 4          |
| Más de 16,000            | 1          |
| **Total**                | **250**    |

a) Construya el histograma de frecuencias relativas respectivo. Note que las clases son de distinta longitud. [10]

```{r}
# Crea un vector con los límites de las clases

clases <- c(0, 50, 150, 300, 500, 1000, 2000, 4000, 8000, 16000, 24000)

# Crea un vector con las frecuencias

frecuencias <- c(3, 54, 62, 43, 39, 24, 11, 9, 4, 1)

# Crea un vector con las frecuencias relativas

frecuencias_relativas <- frecuencias/sum(frecuencias)

# Crea un vector con las longitudes de las clases

longitudes_clases <- diff(clases)


# Crea un vector con los factores de ajuste

factores_ajuste <- 1/longitudes_clases

# Crea un vector con las frecuencias relativas ajustadas

frecuencias_relativas_ajustadas <- frecuencias_relativas * factores_ajuste

# Crea un vector con las frecuencias relativas corregidas

frecuencias_relativas_corregidas <- frecuencias_relativas_ajustadas/sum(frecuencias_relativas_ajustadas)

# Crea un vector con los puntos medios de las clases

puntos_medios_clases <- (clases[-1] + clases[-length(clases)])/2

# Crea un data frame con los datos

df <- data.frame( frecuencias, frecuencias_relativas, frecuencias_relativas_ajustadas, frecuencias_relativas_corregidas, puntos_medios_clases)


# Grafica el histograma
gg1 <-ggplot(df, aes(x = puntos_medios_clases, y = frecuencias_relativas_corregidas)) +
  geom_bar(width = longitudes_clases, stat = "identity", fill = "steelblue") +
  geom_text(aes(label = frecuencias), vjust = -0.5) +
  ggtitle("Histograma de frecuencias relativas") +
  xlab("Pérdida (miles de pesos)") +
  ylab("Frecuencia relativa") +
  theme_minimal()

gg1

ggplot(df[1:6, ], aes(x = puntos_medios_clases, y = frecuencias_relativas_corregidas)) +
  geom_bar(width = longitudes_clases[1:6], stat = "identity", fill = "steelblue") +
  geom_text(aes(label = round(frecuencias_relativas_corregidas,digits = 5)), vjust = -0.5) +
  ggtitle("Histograma de frecuencias relativas (primeras 8 clases)") +
  xlab("Pérdida (miles de pesos)") +
  ylab("Frecuencia relativa") +
  xlim(c(0, 2000)) +  # Limitar el eje x hasta un poco después de 8,000
  theme_minimal()





```



b) A partir de los datos agrupados, ajuste por máxima verosimilitud la Distribución Exponencial Inversa con función de distribución acumulada 
$F(x) = e^{-\frac{\theta}{x}} I(0,\infty)(x), \quad \theta > 0$.
Especifique $\hat{\theta}$ y grafique la densidad ajustada junto con el histograma del inciso anterior. [10]


Por principio de máxima verosiimilitud tenemos que el logaritmo de la función de verosimilitud para datos agrupados está dada por:

$$\ln(L(\theta)) = \sum_{i=1}^{10} n_i \ln\left( F(b_i, \theta) - F(a_i, \theta) \right)$$

```{r}
acumulada <- function(x, theta){
  if(x > 0){
    resultado = exp(-theta/x)
  }else{
    resultado = 0
  }
  return(resultado)
}


verosimilitud <- function(theta){
  for(i in 1:10){
    if(i == 1){
      resultado <- frecuencias[i]*log(acumulada(clases[i+1], theta)-acumulada(clases[i],theta))
    }else{
      resultado <- resultado + frecuencias[i]*log(acumulada(clases[i+1], theta)-acumulada(clases[i],theta))
    }
  }
  return(resultado)
}

neg_verosimilitud <- function(theta) {
  return(-verosimilitud(theta))
}

# Usamos la función optimize para encontrar el valor de theta que maximiza la verosimilitud
# Establece un rango razonable para theta basado en tu conocimiento del problema
resultado <- optimize(neg_verosimilitud, interval=c(0, 9000))



# Compute likelihoods

d_gamma_inv <- function(x, theta) {
  ifelse(x > 0, (theta / (x^(2))) * exp(-theta/x),0)
}

# Valores de x
x_vals <- seq(0, 24000, by = 100)

# Calcular la función de densidad para estos valores
y_vals <- sapply(x_vals, d_gamma_inv, theta =resultado$minimum )

# Graficar

data_to_plot2 <- data.frame(x = x_vals, y = y_vals)

ggplot(data_to_plot2, aes(x = x, y = y)) +
  geom_line(color = "blue") +
  ggtitle(paste("Distribución Gama Inversa: alpha =", 1, ", beta =", resultado$minimum)) +
  xlab("x") +
  ylab("f(x)") +
  theme_minimal() 

# Primero, configura el ggplot base
plot_base <- ggplot() + 
  theme_minimal() +
  xlab("x") + ylab("f(x)")

# Supongo que plot_base y los otros datasets ya están definidos

# Calcula el factor de transformación. Este factor se basará en la relación entre los rangos máximos de tus datos.
max_y_data_to_plot2 <- max(data_to_plot2$y)
max_df <- max(df$frecuencias_relativas_corregidas)

# Factor de transformación
trans_factor <- max_y_data_to_plot2 / max_df

final_plot <- plot_base +
  # Primero traza el histograma
  geom_bar(data = df, aes(x = puntos_medios_clases, y = frecuencias_relativas_corregidas, width = longitudes_clases), stat = "identity", fill = "steelblue") +
  # Luego traza la línea, pero multiplica los valores por el factor de transformación
  geom_line(data = data_to_plot2, aes(x = x, y = y / trans_factor), color = "red") +
  ggtitle(paste("Distribución Gama Inversa: alpha =", 1, ", beta =", resultado$minimum)) +
  # Añade el eje secundario, dividiendo por el factor de transformación para mostrar los valores reales
  scale_y_continuous(sec.axis = sec_axis(~./trans_factor, name = "Eje secundario"))

# Finalmente, muestra el gráfico
print(final_plot)

```

El valor de $\theta$ que maximiza la verosimilitud numéricamente es `r resultado$minimum` .

# Ejercicio 7

Ajuste de una mezcla. Con base en 180 pérdidas de un seguro de vehículos que cubre
automóviles y motocicletas se construyó el siguiente histograma:

```{r}
library(readxl)
library(kableExtra)
ruta <- paste0(getwd(), "/bases/Tarea 3.xlsx")
datosPerdida <- read_excel(ruta, sheet = "7", range = "A8:B188")

hist(datosPerdida$Pérdida)
```

Se quiere ajustar una mezcla de distribuciones Ji-Cuadradas con medias $\nu$ y $2\nu$ con ponderadores $\alpha$ y $1-\alpha$, respectivamente, donde $\nu \in \mathbb{Z^+}$ y $0 < \alpha < 1$.

**a)** A partir de los datos individuales, escriba la función de densidad de la mezcla y verifique que los EMV son $\hat{\alpha} = 0.2468$ y $\hat{\nu} = 30$. (Sugerencia: Optimice numéricamente respecto a ambos parámetros, luego fije $\hat{\nu}$ al entero más cercano y maximice nuevamente para obtener $\hat{\alpha}$). [15]

Primero notamos que la función de verosimilitud está dada por:

$$
L(\nu, \alpha)= \prod_{i=1}^{180} \left[ \alpha \frac{x^{\frac{\nu}{2}-1} e^{-\frac{x}{2}}}{2^{\frac{\nu}{2}} \Gamma\left(\frac{\nu}{2}\right)} + (1-\alpha)\frac{x^{\nu-1} e^{-\frac{x}{2}}}{2^{\nu} \Gamma(\nu)} \right]
$$

Por lo que el logaritmo de la función de verosimilitud está dado por:

$$
\ln(L(\nu, \alpha)) = \sum_{i=1}^{180} \left[ \ln\left( \alpha \frac{x^{\frac{\nu}{2}-1} e^{-\frac{x}{2}}}{2^{\frac{\nu}{2}} \Gamma\left(\frac{\nu}{2}\right)} + (1-\alpha)\frac{x^{\nu-1} e^{-\frac{x}{2}}}{2^{\nu} \Gamma(\nu)} \right) \right]
$$



```{r, warning=FALSE}
# Optimizamos numéricamente respecto a ambos parámetros

mezclaFuncion <- function(x, alpha, nu) {
  alpha * dgamma(x, shape = nu/2, scale = 2) + (1 - alpha) * dgamma(x, shape = nu, scale = 2)+.00000001
}

verosimilitud <- function(alpha, nu, datos) {
  log_mezcla <- log(mezclaFuncion(datos, alpha, nu))
  resultado <- sum(log_mezcla)
  return(resultado)
}

neg_verosimilitud <- function(params, datos) {
  alpha <- params[1]
  nu <- params[2]
  -verosimilitud(alpha, nu, datos)
}

# Suponiendo que tienes un dataframe llamado datosPerdida con una columna 'Pérdida'
datos <- datosPerdida$Pérdida

# Valores iniciales para alpha y nu
valores_iniciales <- c(alpha = 0.5, nu = 2)

# Límites para alpha y nu
# alpha está entre 0 y 1, y nu es mayor que 0
limites <- list(c(0.000001, 10), c(1e-6, 50))

# Utilizar optim con el método "L-BFGS-B" para encontrar los parámetros óptimos
resultado_optim <- optim(
  par = valores_iniciales,
  fn = neg_verosimilitud,
  datos = datos,
  method = "L-BFGS-B",
  lower = sapply(limites, `[`, 1), # Límites inferiores
  upper = sapply(limites, `[`, 2)  # Límites superiores
)

# Los parámetros optimizados
(alpha_optim <- resultado_optim$par[1])
(nu_optim <- resultado_optim$par[2])

# Optimizamos numéricamente respecto a alpha

# Ahora neg_verosimilitud aceptará solo alpha ya que nu es fijo.
neg_verosimilitud <- function(alpha, datos) {
  nu <- 30  # Fijamos nu a 30
  -verosimilitud(alpha, nu, datos)
}

# Suponiendo que tienes un dataframe llamado datosPerdida con una columna 'Pérdida'
datos <- datosPerdida$Pérdida

# Valor inicial solo para alpha
valor_inicial_alpha <- 0.5

# Límite solo para alpha
limites_alpha <- c(0.000001, 1)

# Utilizar optim con el método "L-BFGS-B" para encontrar el parámetro óptimo de alpha
resultado_optim <- optim(
  par = valor_inicial_alpha,
  fn = neg_verosimilitud,
  datos = datos,
  method = "L-BFGS-B",
  lower = limites_alpha[1], # Límite inferior para alpha
  upper = limites_alpha[2]  # Límite superior para alpha
)

# El parámetro optimizado para alpha
alpha_optim <- resultado_optim$par

```

Con lo cual obtenemos que $\hat{\alpha} = `r alpha_optim`$ y $\hat{\nu} = `r 30`$.


**b)** Grafique la función de densidad ajustada junto con el histograma de frecuencias relativas y obtenga los EMV de las modas. [15]

```{r}
# Asumiendo que alpha_optim contiene el valor de alpha optimizado
alpha_emv <- alpha_optim
nu_emv <- 30  # Valor fijo de nu

# 1. Crear una secuencia de valores sobre los que queremos calcular la densidad ajustada
x_values <- seq(min(datosPerdida$Pérdida), max(datosPerdida$Pérdida), length.out = 100)

# 2. Calcular la función de densidad para cada uno de esos valores
densidad_ajustada <- mezclaFuncion(x_values, alpha_emv, nu_emv)

# 3. Crear el histograma con frecuencias relativas
hist(datosPerdida$Pérdida, breaks = 20, probability = TRUE, col = 'gray', border = 'white',
     main = 'Histograma y Densidad Ajustada',
     xlab = 'Pérdida', ylab = 'Densidad')

# 4. Sobreponer la función de densidad ajustada
lines(x_values, densidad_ajustada, col = 'blue', lwd = 2)

# 5. Obtener las modas

# Punto de inicio para la búsqueda de la moda, podría ser el valor medio de los datos, por ejemplo
valor_inicial1 <- 60
valor_inicial2 <- 25
# Necesitamos una función que sea negativa para la función de densidad para poder minimizarla
neg_densidad_mezcla <- function(x, alpha, nu) {
  -mezclaFuncion(x, alpha, nu)
}
# Usar optim para encontrar la moda de la función de densidad
resultado_moda <- optim(
  par = valor_inicial1,
  fn = neg_densidad_mezcla,
  alpha = alpha_emv,
  nu = nu_emv,
  method = "L-BFGS-B",
  lower = min(datosPerdida$Pérdida), # Límite inferior para la búsqueda
  upper = max(datosPerdida$Pérdida)  # Límite superior para la búsqueda
)

# Usar optim para encontrar la moda de la función de densidad
resultado_moda2 <- optim(
  par = valor_inicial2,
  fn = neg_densidad_mezcla,
  alpha = alpha_emv,
  nu = nu_emv,
  method = "L-BFGS-B",
  lower = min(datosPerdida$Pérdida), # Límite inferior para la búsqueda
  upper = max(datosPerdida$Pérdida)  # Límite superior para la búsqueda
)

# La moda es el punto donde la función de densidad es máxima
moda1 <- resultado_moda$par
moda2 <- resultado_moda2$par

```

Con lo cual los valores de las modas son `r moda1` y `r moda2`.


# Ejercicio 8

Pruebas de bondad de ajuste. A continuación se muestran los montos (en miles de
dólares) de 60 siniestros correspondientes a una póliza de incendio:

**a)** Construya un histograma de frecuencias relativas con 6 clases de igual longitud entre $30,000$ y $90,000$ dólares. ¿Qué distribuciones paramétricas podrían ajustar a este perfil de siniestros? [10]

```{r}
library(readxl)
library(kableExtra)
ruta <- paste0(getwd(), "/bases/Tarea 3.xlsx")
datosPerdida <- read_excel(ruta, sheet = "8", range = "L6:M66")

hist(datosPerdida$Monto, 
     breaks = seq(30, 90, length.out = 7), # Define los límites de las clases
     freq = FALSE, # Usa frecuencias relativas en lugar de frecuencias absolutas
     main = "Histograma de Frecuencias Relativas", 
     xlab = "Monto de Pérdida ($)", 
     ylab = "Densidad", 
     col = "lightblue", 
     border = "black")
```

**b)** Si se decide ajustar una Distribución Lognormal con parámetros $\mu \in \mathbb{R}$ y $\sigma^2 > 0$, calcule sus respectivos EMV. Grafique la función de distribución acumulada ajustada junto con la Ojiva. ¿Qué puede concluir? (Sugerencia: Recuerde que estos estimadores fueron deducidos en el Ejercicio E19). [10]

Primero notemos que la función de verosimilitud es de la forma:

$$
L(\mu, \sigma \mid x_1, \ldots, x_n) = \prod_{i=1}^{n} \frac{1}{x_i \sigma \sqrt{2\pi}} \exp\left(-\frac{(\ln x_i - \mu)^2}{2\sigma^2}\right)
$$

y la log-verosimilitud es de la forma:

$$
\ln L(\mu, \sigma \mid \underline{X}_{n}) = -n\ln(\sigma) - \frac{n}{2}\ln(2\pi) - \sum_{i=1}^{n}\ln(x_i) - \frac{1}{2\sigma^2}\sum_{i=1}^{n}(\ln x_i - \mu)^2
$$

```{r}
# Optimizamos numéricamente respecto a ambos parámetros

lognormal <- function(x, mu, sigma) {
  dlnorm(x, meanlog = mu, sdlog = sigma)
}

verosimilitud <- function(mu, sigma, datos) {
  log_ver_ind <- log(lognormal(datos, mu, sigma)+.00000001)
  resultado <- sum(log_ver_ind)
  return(resultado)
}

neg_verosimilitud <- function(params, datos) {
  mu <- params[1]
  sigma <- params[2]
  -verosimilitud(mu, sigma, datos)
}

# Suponiendo que tienes un dataframe llamado datosPerdida con una columna 'Pérdida'
datos <- datosPerdida$Monto

# Valores iniciales para alpha y nu
valores_iniciales <- c(mu = log(40), sigma = 10)

# Límites para alpha y nu
# alpha está entre 0 y 1, y nu es mayor que 0
limites <- list(c(.00001, 90), c(.00001, 90))

# Utilizar optim con el método "L-BFGS-B" para encontrar los parámetros óptimos
resultado_optim <- optim(
  par = valores_iniciales,
  fn = neg_verosimilitud,
  datos = datos,
  method = "L-BFGS-B",
  lower = sapply(limites, `[`, 1), # Límites inferiores
  upper = sapply(limites, `[`, 2)  # Límites superiores
)

# Los parámetros optimizados
mu_optim <- resultado_optim$par[1]
sigma_optim <- resultado_optim$par[2]

```

Con lo cual obtenemos que $\hat{\mu} = `r mu_optim`$ y $\hat{\sigma^{2}} = `r sigma_optim^2`$.


Ahora procederemos a graficar la función de distribución acumulada ajustada junto con la Ojiva.

```{r}
mu_est <- mu_optim
sigma_est <- sigma_optim

# Define una secuencia de valores para los cuales se calculará la ojiva
x_values <- seq(0,100, length.out = 1000)
x_values2 <- seq(30,89, length.out = 1000)
ecdf_datos <- ecdf(datosPerdida$Monto)
# Función de la ojiva

ojiva_lognormal <- function(x){
  particion <- seq(30, 90, length.out = 7)
  for (i in particion){
    if(x <= i & x >= i-10){
      cparticionBefore <- i-10
      cparticionNow <- i
      resultado <-  (cparticionNow - x)*ecdf_datos(cparticionBefore)/(cparticionNow- cparticionBefore) + (x - cparticionBefore)*ecdf_datos(cparticionNow)/(cparticionNow- cparticionBefore)
      break
    }
  } 
  return(resultado)
}
ojiva_empirica_values <- sapply(x_values2, ojiva_lognormal)

lognormal <- function(x) {
  plnorm(x, meanlog = mu_est, sdlog = sigma_est)
}

library(ggplot2)
# Crear el gráfico con ggplot2
ggplot() +
  geom_line(aes(x= x_values, y = lognormal(x_values)), color = "blue") +
  geom_line(aes(x=x_values2, y = ojiva_empirica_values), color = "red") +
  labs(title = 'Log-normal CDF and Empirical CDF', x = 'Monto', y = 'CDF') +
  theme_minimal() +
  theme(legend.position = "bottomright")


```

Tenemos que tomar en cuenta que la ojiva es un estimador de la función de distribución acumulada similar al histograma. Y al paracer nuestra distribución ajustada sí está pareciéndose a la distribución de los datos. Solo cabe resaltar que subestima abtes de los 50,000 y sobreestima después de los 50,000.

**c)** Construya la Gráfica de Probabilidad para el ajuste del inciso b). ¿Qué puede concluir? [10]

```{r}
# Carga la librería necesaria para funciones adicionales si es necesario
# Por ejemplo, para la distribución t, necesitas la función qt de la librería stats
library(stats)

# Genera tus datos
datos <- datosPerdida$Monto

# Distribución empírica
dist_empirica <- ecdf(datos)
datos_empirica <- dist_empirica(datos)
# Distribución teórica
datos_teoricos <- lognormal(datos)

data <- data.frame(datos_empirica, datos_teoricos)

# Crea la gráfica de probabilidad
ggplot(data,aes(x=datos_empirica , y= datos_teoricos)) +
  geom_point() +
  geom_abline(slope = 1, intercept = 0, color = "red") +
  labs(title = "Gráfica de Probabilidad", x = "Distribución Teórica", y = "Distribución Empírica") +
  theme_minimal()

```

Con esta gráfica es mucho más claro y preciso poder decir que la nuestra distribución ajustada se parece a la de los datos muestreados y por tanto no parecen haber grandes desviaciones. Fue un buen ajuste.

**d)** Aplique la Prueba Kolmogorov-Smirnov para validar el ajuste Lognormal del inciso b). Considere un tamaño de error tipo I del 5%. [10]

```{r}
# Realizar la prueba de Kolmogorov-Smirnov
ks_result <- ks.test(datosPerdida$Monto, "plnorm", meanlog = mu_est, sdlog = sigma_est)

# Imprimir el resultado
print(ks_result)
```

Con lo cual no se rechaza la hipótesis nula de que la distribución de los datos es lognormal, es decir, el ajuste fue bueno estadísticamente.

**e)** Aplique la Prueba Ji-Cuadrada para validar el ajuste Lognormal del inciso b). Considere la partición del inciso a) y agregue 2 clases (inicial y final) para abarcar todo el soporte de la Lognormal. Calcule el valor-p para concluir. [10]

```{r}
mis_datos <- datosPerdida$Monto
# Crear una secuencia de intervalos para la tabla de frecuencia
breaks1 <- seq(30, 90, length.out = 7)  # Ajusta el número de intervalos según sea necesario
breaks <- c(-Inf, breaks1, Inf)

# Frecuencias observadas: conteo de datos en cada intervalo
observed <- hist(mis_datos, breaks = breaks, plot = FALSE)$counts

# probabilidades
probabilities <- plnorm(breaks, meanlog = mu_est, sdlog = sigma_est)
diff_prob <- diff(probabilities)

# Realizar la prueba chi-cuadrada
pruebachi <- chisq.test(x = observed, p = diff_prob, rescale.p = TRUE)
kparam <- length(diff_prob) # Número de cajitas
qparam <- 2 # parámetros de la distribución ajustada
df_chi <- kparam - qparam - 1 # Grados de libertad
pvalue <- pchisq(pruebachi$statistic, df = df_chi, lower.tail = FALSE)  # Valor-p
```

Por lo que el estadístico de prueba es `r pruebachi$statistic` y el valor-p es `r pvalue`. Es decir, no hay suficiente evidencia para rechazar la hipótesis nula de que la distribución de los datos es lognormal. Otra vez fue un buen ajuste.

# Ejercicio 9

Selección de modelos. A continuación se muestra el diagrama de tallo y hojas del monto
de las reclamaciones de un seguro que cubre daños ocasionados por fenómenos
hidrometeorológicos:


